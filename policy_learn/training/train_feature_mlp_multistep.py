"""
Feature-MLPå¤šæ­¥é¢„æµ‹è®­ç»ƒè„šæœ¬

åŸºäºé¢„è®­ç»ƒè§¦è§‰ç¼–ç å™¨çš„å¤šæ­¥åŠ¨ä½œåºåˆ—é¢„æµ‹è®­ç»ƒ
"""

import os
import sys
import time
import json
from datetime import datetime
from pathlib import Path

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(project_root)

from policy_learn.models.feature_mlp_multistep import FeatureMLPMultiStep, compute_multistep_losses
from policy_learn.dataset_dataloader.flexible_policy_dataset import FlexiblePolicyDataset
# è®¾ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ä»£ç†æ‰èƒ½è®¿é—®å¤–ç½‘ï¼‰
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7890"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7890"

# è®¾ç½®è¶…æ—¶æ—¶é—´
os.environ["WANDB_HTTP_TIMEOUT"] = "60"
# wandbå¯¼å…¥å’Œé”™è¯¯å¤„ç†
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    print("âš ï¸  wandbæœªå®‰è£…ï¼Œå°†è·³è¿‡åœ¨çº¿æ—¥å¿—è®°å½•")
    WANDB_AVAILABLE = False


def compute_additional_metrics(predictions, targets):
    """
    è®¡ç®—é¢å¤–çš„æŒ‡æ ‡ç”¨äºä¸å•æ­¥è®­ç»ƒå¯¹æ¯”
    
    Args:
        predictions: (B, H, 3) é¢„æµ‹çš„åŠ¨ä½œåºåˆ—
        targets: (B, H, 3) çœŸå®çš„åŠ¨ä½œåºåˆ—
        
    Returns:
        dict: åŒ…å«l1_error, rmseç­‰æŒ‡æ ‡
    """
    # å±•å¹³ä¸º (B*H, 3)
    pred_flat = predictions.view(-1, predictions.size(-1))
    target_flat = targets.view(-1, targets.size(-1))
    
    # L1è¯¯å·®
    l1_error = torch.mean(torch.abs(pred_flat - target_flat)).item()
    
    # RMSE
    mse = torch.mean((pred_flat - target_flat) ** 2)
    rmse = torch.sqrt(mse).item()
    
    # æœ€åä¸€ç»„é¢„æµ‹å’Œç›®æ ‡ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
    last_prediction = predictions[-1, -1].detach().cpu().numpy()  # æœ€åä¸€ä¸ªæ ·æœ¬çš„æœ€åä¸€æ­¥
    last_target = targets[-1, -1].detach().cpu().numpy()
    
    return {
        'l1_error': l1_error,
        'rmse': rmse,
        'last_prediction': last_prediction.tolist(),
        'last_target': last_target.tolist()
    }


def train_multistep_feature_mlp(config):
    """
    è®­ç»ƒå¤šæ­¥Feature-MLPæ¨¡å‹
    
    Args:
        config: è®­ç»ƒé…ç½®å­—å…¸
        
    Returns:
        str: æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„
    """
    print("ğŸ¯ å¤šæ­¥Feature-MLPè®­ç»ƒå¼€å§‹")
    print("ğŸ“Š é…ç½®æ‘˜è¦:")
    print(f"   æ•°æ®æ ¹ç›®å½•: {config['data']['data_root']}")
    print(f"   é¢„æµ‹æ­¥æ•°: {config['model']['horizon']}")
    print(f"   æ‰¹æ¬¡å¤§å°: {config['data']['batch_size']}")
    print(f"   å­¦ä¹ ç‡: {config['training']['lr']}")
    print(f"   è®­ç»ƒè½®æ•°: {config['training']['epochs']}")
    print(f"   è¾“å‡ºç›®å½•: {config['output']['output_dir']}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(project_root, config['output']['output_dir'], f"multistep_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # ä¿å­˜é…ç½®
    config_path = os.path.join(output_dir, 'config.json')
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    # åˆå§‹åŒ–wandb
    if WANDB_AVAILABLE and config.get('wandb', {}).get('enabled', True):
        try:
            wandb.login()
            print("âœ… wandbç™»å½•æˆåŠŸ")
            
            wandb_config = config.get('wandb', {})
            run = wandb.init(
                mode=wandb_config.get('mode', 'online'),
                project=wandb_config.get('project', 'tactile-feature-mlp'),  # ä¸å•æ­¥è®­ç»ƒç›¸åŒçš„é¡¹ç›®
                entity=wandb_config.get('entity', None),
                name=f"feature_mlp_multistep_{timestamp}",  # æ·»åŠ multistepåç¼€
                tags=wandb_config.get('tags', ['feature-mlp', 'multistep', 'tactile']),
                notes=wandb_config.get('notes', 'Multi-step Feature-MLP training with pretrained tactile encoder'),
                config=config,
                dir=output_dir
            )
            print("ğŸ“Š wandbåˆå§‹åŒ–æˆåŠŸ")
            use_wandb = True
        except Exception as e:
            print(f"âš ï¸  wandbåˆå§‹åŒ–å¤±è´¥: {e}")
            use_wandb = False
    else:
        use_wandb = False
    
    print("ğŸš€ å¼€å§‹å¤šæ­¥Feature-MLPè®­ç»ƒ...")
    
    # 1. å‡†å¤‡æ•°æ®é›†
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    
    horizon = config['model']['horizon']
    print(f"ğŸ”„ ä½¿ç”¨æ—¶åºæ¨¡å¼åŠ è½½æ•°æ®ï¼Œåºåˆ—é•¿åº¦: {horizon}")
    
    # ä½¿ç”¨æ—¶åºæ¨¡å¼ç›´æ¥åŠ è½½åºåˆ—æ•°æ®
    train_dataset = FlexiblePolicyDataset(
        data_root=os.path.join(project_root, config['data']['data_root']),
        categories=config['data']['categories'],
        is_train=True,
        train_ratio=config['data']['train_split'],
        start_frame=config['data']['start_frame'],
        normalize_config=config['data']['normalize_config'],
        sequence_mode=True,                    # å¯ç”¨æ—¶åºæ¨¡å¼
        sequence_length=horizon                # åºåˆ—é•¿åº¦ç­‰äºé¢„æµ‹æ­¥æ•°
    )
    
    val_dataset = FlexiblePolicyDataset(
        data_root=os.path.join(project_root, config['data']['data_root']),
        categories=config['data']['categories'],
        is_train=False,
        train_ratio=config['data']['train_split'],
        start_frame=config['data']['start_frame'],
        normalize_config=config['data']['normalize_config'],
        sequence_mode=True,                    # å¯ç”¨æ—¶åºæ¨¡å¼
        sequence_length=horizon                # åºåˆ—é•¿åº¦ç­‰äºé¢„æµ‹æ­¥æ•°
    )
    
    print(f"âœ… æ—¶åºæ•°æ®é›†å‡†å¤‡å®Œæˆ")
    print(f"   è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
    print(f"   éªŒè¯æ ·æœ¬: {len(val_dataset)}")
    print(f"   æ¯ä¸ªæ ·æœ¬åŒ…å« {horizon} æ­¥è¿ç»­æ•°æ®")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['data']['batch_size'],
        shuffle=True,
        num_workers=config['data']['num_workers'],
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['data']['batch_size'],
        shuffle=False,
        num_workers=config['data']['num_workers'],
        pin_memory=True
    )
    
    # 2. åˆ›å»ºæ¨¡å‹
    print("\nğŸ§  åˆ›å»ºå¤šæ­¥é¢„æµ‹æ¨¡å‹...")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    model = FeatureMLPMultiStep(
        feature_dim=config['model']['feature_dim'],
        horizon=config['model']['horizon'],
        action_dim=config['model']['action_dim'],
        hidden_dims=config['model']['hidden_dims'],
        dropout_rate=config['model']['dropout_rate'],
        pretrained_encoder_path=os.path.join(project_root, config['model']['pretrained_encoder_path'])
    ).to(device)
    
    # 3. è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = optim.Adam(
        model.parameters(),
        lr=config['training']['lr'],
        weight_decay=config['training']['weight_decay']
    )
    
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10
    )
    
    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    best_model_path = None
    patience_counter = 0
    
    for epoch in range(config['training']['epochs']):
        print(f"\nğŸ”„ Epoch {epoch+1}/{config['training']['epochs']}")
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_losses = []
        train_metrics = {
            'total_loss': [],
            'mse_loss': [],
            'l1_error': [],         # æ·»åŠ L1è¯¯å·®ï¼Œä¸å•æ­¥è®­ç»ƒå¯¹æ¯”
            'rmse': [],             # æ·»åŠ RMSEï¼Œä¸å•æ­¥è®­ç»ƒå¯¹æ¯”
            'final_step_loss': [],
            'cumulative_error': []
        }
        
        for batch_idx, batch in enumerate(tqdm(train_loader, desc="Training")):
            # æ—¶åºæ¨¡å¼æ•°æ®æ ¼å¼: batch['forces_l/r']: (B, H, 3, 20, 20)
            #                 batch['actions']: (B, H, action_dim)
            
            # å–ç¬¬ä¸€å¸§çš„è§¦è§‰æ•°æ®ä½œä¸ºè¾“å…¥
            forces_l = batch['forces_l'][:, 0].to(device)  # (B, 3, 20, 20)
            forces_r = batch['forces_r'][:, 0].to(device)  # (B, 3, 20, 20)
            
            # æ•´ä¸ªåºåˆ—çš„åŠ¨ä½œä½œä¸ºç›®æ ‡ï¼Œåªå–å‰3ç»´ä½ç½®å¢é‡ (dx, dy, dz)
            target_seq = batch['actions'][:, :, :3].to(device)  # (B, H, 3)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ï¼šç”¨ç¬¬ä¸€å¸§é¢„æµ‹æ•´ä¸ªåºåˆ—
            pred_seq = model(forces_l, forces_r)
            
            # è®¡ç®—æŸå¤±
            loss_dict = compute_multistep_losses(
                pred_seq, target_seq, config['loss']
            )
            
            # è®¡ç®—é¢å¤–æŒ‡æ ‡ï¼ˆä¸å•æ­¥è®­ç»ƒå¯¹æ¯”ï¼‰
            additional_metrics = compute_additional_metrics(pred_seq, target_seq)
            
            loss = loss_dict['total_loss']
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆä¸å•æ­¥è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_losses.append(loss.item())
            for key in train_metrics:
                if key in loss_dict:
                    # ç¡®ä¿å¼ é‡è½¬æ¢ä¸ºCPUæ ‡é‡
                    if isinstance(loss_dict[key], torch.Tensor):
                        train_metrics[key].append(loss_dict[key].detach().cpu().item())
                    else:
                        train_metrics[key].append(loss_dict[key])
                elif key in additional_metrics:
                    # æ·»åŠ é¢å¤–æŒ‡æ ‡
                    train_metrics[key].append(additional_metrics[key])
        
        # è®¡ç®—å¹³å‡è®­ç»ƒæŒ‡æ ‡
        avg_train_loss = np.mean(train_losses)
        avg_train_metrics = {k: np.mean(v) for k, v in train_metrics.items()}
        
        # æ‰“å°è®­ç»ƒç»“æœï¼ˆä¸å•æ­¥è®­ç»ƒæ ¼å¼ä¸€è‡´ï¼‰
        print(f"ğŸ”„ è®­ç»ƒ: Loss={avg_train_loss:.6f}, L1={avg_train_metrics.get('l1_error', 0):.6f}, RMSE={avg_train_metrics.get('rmse', 0):.6f}")
        
        # éªŒè¯é˜¶æ®µ
        if epoch % config['training']['eval_every'] == 0:
            model.eval()
            val_losses = []
            val_metrics = {
                'total_loss': [],
                'mse_loss': [],
                'l1_error': [],         # æ·»åŠ L1è¯¯å·®ï¼Œä¸å•æ­¥è®­ç»ƒå¯¹æ¯”
                'rmse': [],             # æ·»åŠ RMSEï¼Œä¸å•æ­¥è®­ç»ƒå¯¹æ¯”
                'final_step_loss': [],
                'cumulative_error': [],
                'last_prediction': None,  # æœ€åä¸€ç»„é¢„æµ‹
                'last_target': None       # æœ€åä¸€ç»„ç›®æ ‡
            }
            
            with torch.no_grad():
                for batch in tqdm(val_loader, desc="Evaluating"):
                    # å–ç¬¬ä¸€å¸§çš„è§¦è§‰æ•°æ®ä½œä¸ºè¾“å…¥
                    forces_l = batch['forces_l'][:, 0].to(device)  # (B, 3, 20, 20)
                    forces_r = batch['forces_r'][:, 0].to(device)  # (B, 3, 20, 20)
                    
                    # æ•´ä¸ªåºåˆ—çš„åŠ¨ä½œä½œä¸ºç›®æ ‡ï¼Œåªå–å‰3ç»´ä½ç½®å¢é‡ (dx, dy, dz)
                    target_seq = batch['actions'][:, :, :3].to(device)  # (B, H, 3)
                    
                    pred_seq = model(forces_l, forces_r)
                    loss_dict = compute_multistep_losses(
                        pred_seq, target_seq, config['loss']
                    )
                    
                    # è®¡ç®—é¢å¤–æŒ‡æ ‡ï¼ˆä¸å•æ­¥è®­ç»ƒå¯¹æ¯”ï¼‰
                    additional_metrics = compute_additional_metrics(pred_seq, target_seq)
                    
                    val_losses.append(loss_dict['total_loss'].detach().cpu().item())
                    for key in val_metrics:
                        if key in loss_dict:
                            # ç¡®ä¿å¼ é‡è½¬æ¢ä¸ºCPUæ ‡é‡
                            if isinstance(loss_dict[key], torch.Tensor):
                                val_metrics[key].append(loss_dict[key].detach().cpu().item())
                            else:
                                val_metrics[key].append(loss_dict[key])
                        elif key in additional_metrics:
                            # æ·»åŠ é¢å¤–æŒ‡æ ‡æˆ–ä¿ç•™æœ€åä¸€ç»„å€¼
                            if key in ['last_prediction', 'last_target']:
                                val_metrics[key] = additional_metrics[key]
                            else:
                                val_metrics[key].append(additional_metrics[key])
            
            avg_val_loss = np.mean(val_losses)
            avg_val_metrics = {k: np.mean(v) if isinstance(v, list) else v for k, v in val_metrics.items()}
            
            print(f"ğŸ“Š éªŒè¯ç»“æœ:")
            print(f"   Loss: {avg_val_loss:.6f}")
            print(f"   L1: {avg_val_metrics.get('l1_error', 0):.6f}")
            print(f"   RMSE: {avg_val_metrics.get('rmse', 0):.6f}")
            print(f"   Final step loss: {avg_val_metrics.get('final_step_loss', 0):.6f}")
            print(f"   Last prediction: {avg_val_metrics.get('last_prediction', [])}")
            print(f"   Last target: {avg_val_metrics.get('last_target', [])}")
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(avg_val_loss)
            
            # è®°å½•åˆ°wandb (ä¸å•æ­¥è®­ç»ƒç›¸åŒæ ¼å¼)
            if use_wandb:
                log_dict = {
                    'epoch': epoch,
                    'train/loss': avg_train_loss,
                    'train/l1_error': avg_train_metrics.get('l1_error', 0),
                    'train/rmse': avg_train_metrics.get('rmse', 0),
                    'val/loss': avg_val_loss,
                    'val/l1_error': avg_val_metrics.get('l1_error', 0),
                    'val/rmse': avg_val_metrics.get('rmse', 0),
                    'learning_rate': optimizer.param_groups[0]['lr'],
                    # å¤šæ­¥ç‰¹æœ‰çš„æŒ‡æ ‡
                    'train/final_step': avg_train_metrics.get('final_step_loss', 0),
                    'val/final_step': avg_val_metrics.get('final_step_loss', 0),
                    'val/cumulative_error': avg_val_metrics.get('cumulative_error', 0)
                }
                wandb.log(log_dict)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_loss:
                best_loss = avg_val_loss
                patience_counter = 0
                
                best_model_path = os.path.join(output_dir, 'best_multistep_model.pt')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': best_loss,
                    'config': config
                }, best_model_path)
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_model_path}")
            else:
                patience_counter += 1
                
            # æ—©åœæ£€æŸ¥
            if patience_counter >= config['training']['patience']:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œå·²ç­‰å¾…{patience_counter}è½®æ— æ”¹å–„")
                break
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % config['training']['save_every'] == 0:
            checkpoint_path = os.path.join(output_dir, f'checkpoint_epoch_{epoch}.pt')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_train_loss,
                'config': config
            }, checkpoint_path)
    
    # è®­ç»ƒå®Œæˆ
    if use_wandb:
        wandb.finish()
    
    print(f"\nğŸ‰ å¤šæ­¥è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ˆ æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.4f}")
    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹: {best_model_path}")
    
    return best_model_path


def main(config=None):
    """ä¸»å‡½æ•°"""
    if config is None:
        # é»˜è®¤é…ç½®
        config = {
            'data': {
                'data_root': 'datasets/data25.7_aligned',
                'categories': [
                    "cir_lar", "cir_med", "cir_sma",
                    "rect_lar", "rect_med", "rect_sma", 
                    "tri_lar", "tri_med", "tri_sma"
                ],
                'train_split': 0.8,
                'batch_size': 16,  # å¤šæ­¥é¢„æµ‹æ•°æ®æ›´å¤æ‚ï¼Œå‡å°batch size
                'num_workers': 4,
                'start_frame': 0,
                'normalize_config': {
                    'forces': 'zscore',
                    'actions': 'minmax',
                    'end_states': None,
                    'resultants': None
                }
            },
            'model': {
                'feature_dim': 128,
                'horizon': 5,      # é¢„æµ‹5æ­¥
                'action_dim': 3,   # (dx, dy, dz)
                'hidden_dims': [256, 128],
                'dropout_rate': 0.25,
                'pretrained_encoder_path': 'tactile_representation/prototype_library/cnnae_crt_128.pt'
            },
            'loss': {
                'loss_type': 'huber',
                'huber_delta': 1.0,
                'step_weights': [2.0, 1.5, 1.2, 1.0, 1.0]  # ç­‰æƒé‡ï¼Œæˆ–è€…å¯ä»¥è®¾ç½®å¦‚ [1.0, 1.2, 1.5, 2.0, 3.0] é‡ç‚¹å…³æ³¨åç»­æ­¥éª¤
            },
            'training': {
                'epochs': 100,
                'lr': 5e-5,    # å¤šæ­¥é¢„æµ‹ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡
                'weight_decay': 1e-4,
                'eval_every': 2,
                'save_every': 10,
                'patience': 20
            },
            'output': {
                'output_dir': 'policy_learn/checkpoints'
            },
            'wandb': {
                'enabled': True,
                'mode': 'online',
                'project': 'tactile-feature-mlp',  # ä¸å•æ­¥è®­ç»ƒç›¸åŒé¡¹ç›®
                'entity': None,
                'tags': ['feature-mlp', 'multistep', 'tactile', 'behavior-cloning'],  # ä¸å•æ­¥è®­ç»ƒç›¸ä¼¼æ ‡ç­¾
                'notes': 'Multi-step Feature-MLP training with pretrained tactile encoder and flexible dataset'
            }
        }
    
    # æ£€æŸ¥è·¯å¾„
    data_path = os.path.join(project_root, config['data']['data_root'])
    pretrained_path = os.path.join(project_root, config['model']['pretrained_encoder_path'])
    
    print(f"ğŸ¯ å¤šæ­¥Feature-MLPè®­ç»ƒé…ç½®:")
    print(f"   é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"   æ•°æ®è·¯å¾„: {data_path}")
    print(f"   é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
    print(f"   é¢„æµ‹æ­¥æ•°: {config['model']['horizon']}")
    print(f"   è¾“å‡ºç›®å½•: {os.path.join(project_root, config['output']['output_dir'])}")
    
    if os.path.exists(data_path):
        print(f"âœ… æ•°æ®è·¯å¾„å­˜åœ¨")
    else:
        print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
        return
    
    if os.path.exists(pretrained_path):
        print(f"âœ… é¢„è®­ç»ƒæƒé‡å­˜åœ¨")
    else:
        print(f"âš ï¸  é¢„è®­ç»ƒæƒé‡ä¸å­˜åœ¨: {pretrained_path}")
        print("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–")
        config['model']['pretrained_encoder_path'] = ''
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "="*60)
    best_model_path = train_multistep_feature_mlp(config)
    print("="*60)
    
    return best_model_path


if __name__ == '__main__':
    main()
