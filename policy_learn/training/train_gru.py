"""
GRUæ—¶åºç­–ç•¥æ¨¡å‹è®­ç»ƒè„šæœ¬
åŸºäºflexible_policy_dataset.pyæ•°æ®å¤„ç†å’Œä½ç»´ç­–ç•¥ï¼ˆæ—¶åºï¼‰è¦æ±‚
è¾“å…¥: resultant_force[6] + resultant_moment[6] {t}
è¾“å‡º: delta_action_nextstep[3]
"""

import os
import sys
import torch
import torch.nn.functional as F
import numpy as np
from torch.utils.data import DataLoader
from tqdm import tqdm
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
sys.path.append(project_root)

from utils.logging import Logger
from utils.visualization import plot_all_losses_single_plot
from policy_learn.dataset_dataloader.flexible_policy_dataset import create_flexible_datasets
from policy_learn.models.gru import create_tactile_policy_gru, compute_gru_policy_losses

# è®¾ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ä»£ç†æ‰èƒ½è®¿é—®å¤–ç½‘ï¼‰
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7890"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7890"

# è®¾ç½®è¶…æ—¶æ—¶é—´
os.environ["WANDB_HTTP_TIMEOUT"] = "60"
# wandbå¯¼å…¥å’Œé”™è¯¯å¤„ç†
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    print("âš ï¸  wandbæœªå®‰è£…ï¼Œå°†è·³è¿‡åœ¨çº¿æ—¥å¿—è®°å½•")
    WANDB_AVAILABLE = False


def compute_additional_metrics(predictions, targets):
    """
    è®¡ç®—é¢å¤–çš„æŒ‡æ ‡ç”¨äºwandbè®°å½•
    
    Args:
        predictions: é¢„æµ‹çš„åŠ¨ä½œåºåˆ— (B, T-1, 3)
        targets: çœŸå®çš„åŠ¨ä½œåºåˆ— (B, T-1, 3)
        
    Returns:
        dict: åŒ…å«l1_errorç­‰æŒ‡æ ‡
    """
    # å±•å¹³ä¸º (B*(T-1), 3)
    pred_flat = predictions.view(-1, predictions.size(-1))
    target_flat = targets.view(-1, targets.size(-1))
    
    # L1è¯¯å·®
    l1_error = torch.mean(torch.abs(pred_flat - target_flat)).item()
    
    # æœ€åä¸€ç»„é¢„æµ‹å’Œç›®æ ‡ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
    last_prediction = predictions[-1, -1].detach().cpu().numpy()  # æœ€åä¸€ä¸ªæ ·æœ¬çš„æœ€åä¸€æ­¥
    last_target = targets[-1, -1].detach().cpu().numpy()
    
    return {
        'l1_error': l1_error,
        'last_prediction': last_prediction.tolist(),
        'last_target': last_target.tolist()
    }
    
    
def prepare_gru_input_from_flexible_dataset(batch_data):
    """
    ä»FlexiblePolicyDatasetæ‰¹æ¬¡ä¸­å‡†å¤‡GRUæ¨¡å‹çš„è¾“å…¥
    
    Args:
        batch_data: æ¥è‡ªFlexiblePolicyDatasetçš„æ‰¹æ¬¡æ•°æ®ï¼ˆæ—¶åºæ¨¡å¼ï¼‰
    
    Returns:
        dict: GRUæ¨¡å‹çš„è¾“å…¥å­—å…¸
    """
    # åˆå¹¶å·¦å³æ‰‹çš„åˆåŠ›å’ŒåˆåŠ›çŸ©
    resultant_force = torch.cat([
        batch_data['resultant_force_l'], 
        batch_data['resultant_force_r']
    ], dim=-1)  # (B, T, 6) - æ—¶åºæ¨¡å¼
    
    resultant_moment = torch.cat([
        batch_data['resultant_moment_l'], 
        batch_data['resultant_moment_r']
    ], dim=-1)  # (B, T, 6) - æ—¶åºæ¨¡å¼
    
    # è®¡ç®—ç›®æ ‡åŠ¨ä½œå¢é‡åºåˆ—
    actions = batch_data['actions']  # (B, T, action_dim) - æ³¨æ„flexible_policy_datasetæ—¶åºæ¨¡å¼è¿”å›'actions'è€Œä¸æ˜¯'action'
    current_actions = actions[:, :-1, :3]  # å‰T-1æ­¥çš„ä½ç½® (B, T-1, 3)
    next_actions = actions[:, 1:, :3]  # åT-1æ­¥çš„ä½ç½® (B, T-1, 3)
    target_delta_action = next_actions - current_actions  # (B, T-1, 3)
    
    # å¯¹åº”è°ƒæ•´è¾“å…¥ç‰¹å¾ï¼ˆå–å‰T-1æ­¥ï¼‰
    resultant_force = resultant_force[:, :-1]  # (B, T-1, 6)
    resultant_moment = resultant_moment[:, :-1]  # (B, T-1, 6)
    
    return {
        'resultant_force': resultant_force,
        'resultant_moment': resultant_moment,
        'target_delta_action': target_delta_action
    }


def train_tactile_policy_gru(config):
    """
    è®­ç»ƒGRUç­–ç•¥æ¨¡å‹
    Args:
        config: é…ç½®å­—å…¸
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = config['output']['output_dir']
    os.makedirs(output_dir, exist_ok=True)
    log_file = os.path.join(output_dir, "train_tactile_policy_gru.log")
    sys.stdout = Logger(log_file)
    
    # åˆå§‹åŒ–wandb
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if WANDB_AVAILABLE and config.get('wandb', {}).get('enabled', True):
        try:
            wandb.login()
            print("âœ… wandbç™»å½•æˆåŠŸ")
            
            wandb_config = config.get('wandb', {})
            run = wandb.init(
                mode=wandb_config.get('mode', 'online'),
                project=wandb_config.get('project', 'tactile-gru-policy'),
                entity=wandb_config.get('entity', None),
                name=f"gru_policy_{timestamp}",
                tags=wandb_config.get('tags', ['gru', 'policy', 'tactile']),
                notes=wandb_config.get('notes', 'GRU policy training with flexible dataset'),
                config=config,
                dir=output_dir
            )
            print("ğŸ“Š wandbåˆå§‹åŒ–æˆåŠŸ")
            use_wandb = True
        except Exception as e:
            print(f"âš ï¸  wandbåˆå§‹åŒ–å¤±è´¥: {e}")
            use_wandb = False
    else:
        use_wandb = False
    
    print("=" * 60)
    print("GRU Policy Training (Flexible Dataset)")
    print(f"Output Directory: {output_dir}")
    print(f"Data Root: {config['data']['data_root']}")
    print(f"Sequence Length: {config['data']['sequence_length']}")
    print(f"Batch Size: {config['training']['batch_size']}")
    print(f"Epochs: {config['training']['epochs']}")
    print(f"Learning Rate: {config['training']['lr']}")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ï¼ˆæ—¶åºæ¨¡å¼ï¼‰
    train_dataset, test_dataset = create_flexible_datasets(
        data_root=config['data']['data_root'],
        categories=config['data']['categories'],
        train_ratio=config['data']['train_ratio'],
        random_seed=config['data']['random_seed'],
        start_frame=config['data']['start_frame'],
        use_end_states=config['data']['use_end_states'],
        use_forces=config['data']['use_forces'],
        use_resultants=config['data']['use_resultants'],
        normalize_config=config['data'].get('normalize_config', None),
        sequence_mode=True,  # GRUä½¿ç”¨æ—¶åºæ¨¡å¼
        sequence_length=config['data']['sequence_length']
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
    if len(train_dataset) == 0:
        print("âŒ é”™è¯¯ï¼šè®­ç»ƒé›†ä¸ºç©ºï¼")
        print(f"   æ•°æ®è·¯å¾„: {config['data']['data_root']}")
        print(f"   æ•°æ®ç±»åˆ«: {config['data']['categories']}")
        print("   è¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œç±»åˆ«è®¾ç½®")
        return None, None, None
        
    if len(test_dataset) == 0:
        print("âŒ é”™è¯¯ï¼šæµ‹è¯•é›†ä¸ºç©ºï¼")
        print("   è¯·æ£€æŸ¥train_ratioè®¾ç½®æˆ–æ•°æ®é‡")
        return None, None, None
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['training']['batch_size'], 
        shuffle=True, 
        num_workers=config['data']['num_workers'], 
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=config['training']['batch_size'], 
        shuffle=False, 
        num_workers=config['data']['num_workers'], 
        pin_memory=True
    )

    # åˆ›å»ºæ¨¡å‹
    model = create_tactile_policy_gru(config['model']).cuda()
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=config['training']['lr'], 
        weight_decay=config['training']['weight_decay']
    )
    
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10
    )

    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    patience_counter = 0
    patience = config['training']['patience']
    
    # è®°å½•æŸå¤±å†å²
    train_history = {'epoch': [], 'loss': []}
    test_history = {'epoch': [], 'loss': []}

    for epoch in range(1, config['training']['epochs'] + 1):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_total_loss = 0
        train_samples = 0
        train_l1_errors = []
        train_last_predictions = []
        train_last_targets = []
        
        for batch in tqdm(train_loader, desc=f"Epoch {epoch}/{config['training']['epochs']} [Train]"):
            # å‡†å¤‡è¾“å…¥æ•°æ®
            gru_inputs = prepare_gru_input_from_flexible_dataset(batch)
            
            # å°†æ•°æ®ç§»åˆ°GPU
            for key in gru_inputs:
                if isinstance(gru_inputs[key], torch.Tensor):
                    gru_inputs[key] = gru_inputs[key].cuda()
            
            # å‰å‘ä¼ æ’­
            resultant_force = gru_inputs['resultant_force']  # (B, T-1, 6)
            resultant_moment = gru_inputs['resultant_moment']  # (B, T-1, 6)
            
            # åˆå§‹åŒ–éšè—çŠ¶æ€
            batch_size = resultant_force.size(0)
            hidden = model.init_hidden(batch_size, resultant_force.device)
            
            outputs, _ = model(resultant_force, resultant_moment, hidden)
            
            # è®¡ç®—æŸå¤±
            loss, metrics = compute_gru_policy_losses(gru_inputs, outputs, config['loss'])
            
            # è®¡ç®—é¢å¤–æŒ‡æ ‡
            additional_metrics = compute_additional_metrics(outputs, gru_inputs['target_delta_action'])
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            # ç´¯ç§¯æŸå¤±å’ŒæŒ‡æ ‡
            current_batch_size = resultant_force.size(0) * resultant_force.size(1)  # B * (T-1)
            train_total_loss += loss.item() * current_batch_size
            train_samples += current_batch_size
            train_l1_errors.append(additional_metrics['l1_error'])
            train_last_predictions.append(additional_metrics['last_prediction'])
            train_last_targets.append(additional_metrics['last_target'])
        
        # è®¡ç®—è®­ç»ƒå¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
        train_avg_loss = train_total_loss / train_samples
        train_avg_l1 = np.mean(train_l1_errors)
        train_last_pred = train_last_predictions[-1] if train_last_predictions else None
        train_last_tgt = train_last_targets[-1] if train_last_targets else None
        
        # æµ‹è¯•é˜¶æ®µ
        model.eval()
        test_total_loss = 0
        test_samples = 0
        test_l1_errors = []
        test_last_predictions = []
        test_last_targets = []
        all_predictions = []  # æ”¶é›†æ‰€æœ‰é¢„æµ‹å€¼
        all_targets = []      # æ”¶é›†æ‰€æœ‰ç›®æ ‡å€¼
        
        with torch.no_grad():
            for batch in tqdm(test_loader, desc=f"Epoch {epoch}/{config['training']['epochs']} [Test]"):
                # å‡†å¤‡è¾“å…¥æ•°æ®
                gru_inputs = prepare_gru_input_from_flexible_dataset(batch)
                
                # å°†æ•°æ®ç§»åˆ°GPU
                for key in gru_inputs:
                    if isinstance(gru_inputs[key], torch.Tensor):
                        gru_inputs[key] = gru_inputs[key].cuda()
                
                # å‰å‘ä¼ æ’­
                resultant_force = gru_inputs['resultant_force']  # (B, T-1, 6)
                resultant_moment = gru_inputs['resultant_moment']  # (B, T-1, 6)
                
                # åˆå§‹åŒ–éšè—çŠ¶æ€
                batch_size = resultant_force.size(0)
                hidden = model.init_hidden(batch_size, resultant_force.device)
                
                outputs, _ = model(resultant_force, resultant_moment, hidden)
                
                # è®¡ç®—æŸå¤±
                loss, metrics = compute_gru_policy_losses(gru_inputs, outputs, config['loss'])
                
                # è®¡ç®—é¢å¤–æŒ‡æ ‡
                additional_metrics = compute_additional_metrics(outputs, gru_inputs['target_delta_action'])
                
                # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’Œç›®æ ‡å€¼ (å±•å¹³ä¸º (B*(T-1), 3))
                pred_flat = outputs.view(-1, outputs.size(-1)).detach().cpu().numpy()  # (B*(T-1), 3)
                target_flat = gru_inputs['target_delta_action'].view(-1, gru_inputs['target_delta_action'].size(-1)).detach().cpu().numpy()  # (B*(T-1), 3)
                all_predictions.extend(pred_flat.tolist())
                all_targets.extend(target_flat.tolist())
                
                # ç´¯ç§¯æŸå¤±å’ŒæŒ‡æ ‡
                current_batch_size = resultant_force.size(0) * resultant_force.size(1)  # B * (T-1)
                test_total_loss += loss.item() * current_batch_size
                test_samples += current_batch_size
                test_l1_errors.append(additional_metrics['l1_error'])
                test_last_predictions.append(additional_metrics['last_prediction'])
                test_last_targets.append(additional_metrics['last_target'])
        
        # è®¡ç®—æµ‹è¯•å¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
        test_avg_loss = test_total_loss / test_samples
        test_avg_l1 = np.mean(test_l1_errors)
        test_last_pred = test_last_predictions[-1] if test_last_predictions else None
        test_last_tgt = test_last_targets[-1] if test_last_targets else None
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(test_avg_loss)
        current_lr = optimizer.param_groups[0]['lr']
        
        # æ‰“å°è®­ç»ƒä¿¡æ¯
        print(f"Epoch {epoch}/{config['training']['epochs']}")
        print(f"  Train Loss: {train_avg_loss:.6f}, L1: {train_avg_l1:.6f}")
        print(f"  Test Loss: {test_avg_loss:.6f}, L1: {test_avg_l1:.6f}")
        print(f"  Learning Rate: {current_lr:.6e}")
        print(f"  Last prediction: {test_last_pred}")
        print(f"  Last target: {test_last_tgt}")
        print("-" * 50)
        
        # è¾“å‡ºæ‰€æœ‰targetå’Œpredictionå€¼åˆ°logæ–‡ä»¶
        print("=== All Target and Prediction Values ===")
        for i, (target, prediction) in enumerate(zip(all_targets, all_predictions)):
            # æ¯è¡Œæ ¼å¼: target[0] target[1] target[2] prediction[0] prediction[1] prediction[2]
            print(f"{target[0]:.6f} {target[1]:.6f} {target[2]:.6f} {prediction[0]:.6f} {prediction[1]:.6f} {prediction[2]:.6f}")
        print("=== End of Target and Prediction Values ===")
        
        # è®°å½•åˆ°wandb
        if use_wandb:
            log_dict = {
                'epoch': epoch,
                'train/loss': train_avg_loss,
                'train/l1_error': train_avg_l1,
                'test/loss': test_avg_loss,
                'test/l1_error': test_avg_l1,
                'learning_rate': current_lr
            }
            # æ·»åŠ æœ€åä¸€ç»„é¢„æµ‹å’Œç›®æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if test_last_pred is not None:
                log_dict['test/last_prediction'] = test_last_pred
            if test_last_tgt is not None:
                log_dict['test/last_target'] = test_last_tgt
            wandb.log(log_dict)
        
        # è®°å½•æŸå¤±å†å²
        train_history['epoch'].append(epoch)
        train_history['loss'].append(train_avg_loss)
        test_history['epoch'].append(epoch)
        test_history['loss'].append(test_avg_loss)
        
        # æ—©åœæ£€æŸ¥
        if test_avg_loss < best_loss:
            best_loss = test_avg_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': epoch,
                'train_loss': train_avg_loss,
                'test_loss': test_avg_loss,
                'config': config
            }, os.path.join(output_dir, "best_model.pt"))
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Test Loss: {best_loss:.6f})")
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f"æ—©åœï¼š{patience} ä¸ªepochæ²¡æœ‰æ”¹å–„")
            break
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': epoch,
        'train_loss': train_avg_loss,
        'test_loss': test_avg_loss,
        'config': config
    }, os.path.join(output_dir, "final_model.pt"))
    
    # ä¿å­˜è®­ç»ƒæŸå¤±æ›²çº¿
    combined_history = {
        'epoch': train_history['epoch'],
        'train_loss': train_history['loss'],
        'test_loss': test_history['loss']
    }
    
    plot_all_losses_single_plot(
        combined_history, 
        save_path=os.path.join(output_dir, "training_loss_curves.png"),
        title="GRU Policy Training Loss (Flexible Dataset)"
    )
    
    # ä¿å­˜æŸå¤±å†å²æ•°æ®
    np.save(os.path.join(output_dir, "train_loss_history.npy"), train_history)
    np.save(os.path.join(output_dir, "test_loss_history.npy"), test_history)
    
    # ç»“æŸwandbè®°å½•
    if use_wandb:
        wandb.finish()
    
    print("âœ… GRUç­–ç•¥æ¨¡å‹è®­ç»ƒå®Œæˆ!")
    return model, train_history, test_history


if __name__ == '__main__':
    # é»˜è®¤é…ç½®
    config = {
        'data': {
            'data_root': os.path.join(project_root, 'datasets', 'data25.7_aligned'),  # ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½• + ç›¸å¯¹è·¯å¾„
            'categories': [
                "cir_lar", "cir_med", "cir_sma",
                "rect_lar", "rect_med", "rect_sma", 
                "tri_lar", "tri_med", "tri_sma"
            ],
            'sequence_length': 10,
            'train_ratio': 0.8,
            'random_seed': 42,
            'start_frame': 0,
            'use_end_states': True,
            'use_forces': False,  # GRUä¸éœ€è¦åŸå§‹åŠ›æ•°æ®
            'use_resultants': True,  # éœ€è¦åˆåŠ›/çŸ©æ•°æ®
            'normalize_config': {
                'actions': 'minmax',
                'resultants': 'zscore',  # z-scoreæ ‡å‡†åŒ–åˆåŠ›/çŸ©æ•°æ®
                'end_states': None
            },
            'num_workers': 8
        },
        'model': {
            'input_dim': 12,  # resultant_force[6] + resultant_moment[6]
            'output_dim': 3,  # delta_action_nextstep[3]
            'hidden_dim': 256,
            'num_layers': 2,
            'dropout': 0.1,
            'use_normalization': True  # z-scoreæ ‡å‡†åŒ–
        },
        'loss': {
            'l2_weight': 1.0,           # L2æŸå¤±æƒé‡ï¼ˆä¸»è¦æŸå¤±ï¼‰
            'huber_weight': 0.0,        # HuberæŸå¤±æƒé‡ï¼ˆå¯é€‰ï¼Œè®¾ä¸º0è¡¨ç¤ºä¸ä½¿ç”¨ï¼‰
            'delta_u_weight': 0.1,      # Î”uå¹³æ»‘æ­£åˆ™æƒé‡
            'jerk_weight': 0.05         # jerkå¹³æ»‘æ­£åˆ™æƒé‡
        },
        'training': {
            'batch_size': 16,
            # 'epochs': 100,
            'epochs': 50,
            'lr': 1e-3,
            'weight_decay': 1e-4,
            'patience': 20
        },
        'output': {
            'output_dir': os.path.join(project_root, "policy_learn/checkpoints", 
                                     datetime.now().strftime("%Y-%m-%d_%H-%M-%S") + "_gru_policy")
        },
        'wandb': {
            'enabled': True,
            'mode': 'online',
            'project': 'tactile-feature-mlp',
            'entity': None,
            'tags': ['gru', 'policy', 'tactile', 'flexible-dataset'],
            'notes': 'GRU policy training with flexible dataset and L1 metrics'
        }
    }
    
    train_tactile_policy_gru(config)