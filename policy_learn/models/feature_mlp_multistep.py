"""
Feature-MLPå¤šæ­¥é¢„æµ‹æ¨¡å‹

åŸºäºé¢„è®­ç»ƒè§¦è§‰ç¼–ç å™¨çš„å¤šæ­¥åŠ¨ä½œåºåˆ—é¢„æµ‹
è¾“å…¥å•å¸§è§¦è§‰æ•°æ®ï¼Œè¾“å‡ºHæ­¥ä½ç½®å¢é‡åºåˆ—
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥å¯¼å…¥è§¦è§‰ç¼–ç å™¨
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(project_root)

try:
    from tactile_representation.Prototype_Discovery.models.cnn_autoencoder import TactileCNNAutoencoder
except ImportError:
    print("âš ï¸  æ— æ³•å¯¼å…¥TactileCNNAutoencoderï¼Œå°†ä½¿ç”¨ç®€å•æ›¿ä»£")
    TactileCNNAutoencoder = None


class FeatureMLPMultiStep(nn.Module):
    """
    Feature-MLPå¤šæ­¥é¢„æµ‹æ¨¡å‹ï¼šåŸºäºé¢„è®­ç»ƒè§¦è§‰ç‰¹å¾çš„å¤šæ­¥è¡Œä¸ºé¢„æµ‹
    
    æ¶æ„ï¼š
    1. é¢„è®­ç»ƒCNNç¼–ç å™¨æå–å·¦å³æ‰‹è§¦è§‰ç‰¹å¾ (2 Ã— 128ç»´)
    2. ç‰¹å¾è¿æ¥åè¾“å…¥MLP (256 â†’ 256 â†’ 128 â†’ H*3)
    3. è¾“å‡ºé‡å¡‘ä¸ºHæ­¥ä½ç½®å¢é‡åºåˆ— (B, H, 3)
    """
    
    def __init__(self, 
                 feature_dim=128,           # å•æ‰‹ç‰¹å¾ç»´åº¦
                 horizon=5,                 # é¢„æµ‹æ­¥æ•°H
                 action_dim=3,              # å•æ­¥åŠ¨ä½œç»´åº¦ (dx, dy, dz)
                 hidden_dims=[256, 128],    # éšè—å±‚ç»´åº¦
                 dropout_rate=0.25,         # Dropoutç‡
                 pretrained_encoder_path=None):
        """
        Args:
            feature_dim: å•æ‰‹è§¦è§‰ç‰¹å¾ç»´åº¦
            horizon: é¢„æµ‹çš„æ—¶é—´æ­¥æ•°H
            action_dim: å•æ­¥åŠ¨ä½œç»´åº¦
            hidden_dims: MLPéšè—å±‚ç»´åº¦åˆ—è¡¨
            dropout_rate: Dropoutæ¯”ç‡
            pretrained_encoder_path: é¢„è®­ç»ƒç¼–ç å™¨æƒé‡è·¯å¾„
        """
        super(FeatureMLPMultiStep, self).__init__()
        
        self.feature_dim = feature_dim
        self.horizon = horizon
        self.action_dim = action_dim
        self.hidden_dims = hidden_dims
        self.dropout_rate = dropout_rate
        
        # åˆå§‹åŒ–é¢„è®­ç»ƒè§¦è§‰ç¼–ç å™¨
        if TactileCNNAutoencoder is not None:
            print("ğŸ”— åˆå§‹åŒ–é¢„è®­ç»ƒè§¦è§‰ç¼–ç å™¨...")
            self.tactile_encoder = TactileCNNAutoencoder(
                in_channels=3, 
                latent_dim=feature_dim
            )
            
            # åŠ è½½é¢„è®­ç»ƒæƒé‡
            if pretrained_encoder_path is not None and os.path.exists(pretrained_encoder_path):
                print(f"åŠ è½½é¢„è®­ç»ƒè§¦è§‰ç¼–ç å™¨: {pretrained_encoder_path}")
                checkpoint = torch.load(pretrained_encoder_path, map_location='cpu')
                
                # æ£€æŸ¥checkpointæ ¼å¼ï¼Œæå–æ¨¡å‹çŠ¶æ€å­—å…¸
                if isinstance(checkpoint, dict):
                    if 'model_state_dict' in checkpoint:
                        # æ ‡å‡†è®­ç»ƒcheckpointæ ¼å¼
                        model_state = checkpoint['model_state_dict']
                        print("ğŸ“¦ æ£€æµ‹åˆ°è®­ç»ƒcheckpointæ ¼å¼ï¼Œæå–model_state_dict")
                    elif 'state_dict' in checkpoint:
                        # å¦ä¸€ç§å¸¸è§æ ¼å¼
                        model_state = checkpoint['state_dict']
                        print("ğŸ“¦ æ£€æµ‹åˆ°state_dictæ ¼å¼")
                    else:
                        # ç›´æ¥çš„çŠ¶æ€å­—å…¸
                        model_state = checkpoint
                        print("ğŸ“¦ æ£€æµ‹åˆ°ç›´æ¥çŠ¶æ€å­—å…¸æ ¼å¼")
                else:
                    model_state = checkpoint
                
                # åŠ è½½çŠ¶æ€å­—å…¸
                self.tactile_encoder.load_state_dict(model_state, strict=True)
                print("âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡")
                
                # æ‰“å°checkpointä¿¡æ¯
                if isinstance(checkpoint, dict) and 'epoch' in checkpoint:
                    print(f"ğŸ“Š é¢„è®­ç»ƒæ¨¡å‹ä¿¡æ¯: epoch {checkpoint['epoch']}")
                    
            else:
                print("âš ï¸  é¢„è®­ç»ƒæƒé‡è·¯å¾„æ— æ•ˆï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
            
            # å†»ç»“ç‰¹å¾æå–å™¨å‚æ•°
            for param in self.tactile_encoder.parameters():
                param.requires_grad = False
            print("ğŸ”’ ç‰¹å¾æå–å™¨å‚æ•°å·²å†»ç»“")
        else:
            print("âŒ æ— æ³•å¯¼å…¥CNNç¼–ç å™¨ï¼Œå°†ä½¿ç”¨éšæœºç‰¹å¾")
            self.tactile_encoder = None
        
        # æ„å»ºå¤šæ­¥é¢„æµ‹MLPç½‘ç»œ
        # è¾“å…¥ç»´åº¦: å·¦å³æ‰‹ç‰¹å¾è¿æ¥ = feature_dim * 2
        input_dim = feature_dim * 2
        
        layers = []
        prev_dim = input_dim
        
        # éšè—å±‚
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout_rate)
            ])
            prev_dim = hidden_dim
        
        # è¾“å‡ºå±‚ï¼šé¢„æµ‹Hæ­¥åŠ¨ä½œåºåˆ—
        output_dim = horizon * action_dim
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.mlp = nn.Sequential(*layers)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"ğŸ§  æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
    
    def forward(self, forces_l, forces_r):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            forces_l: å·¦æ‰‹è§¦è§‰åŠ›æ•°æ® (B, 3, 20, 20)
            forces_r: å³æ‰‹è§¦è§‰åŠ›æ•°æ® (B, 3, 20, 20)
            
        Returns:
            pred_seq: é¢„æµ‹çš„Hæ­¥åŠ¨ä½œåºåˆ— (B, H, 3) - [[dx1,dy1,dz1], [dx2,dy2,dz2], ...]
        """
        batch_size = forces_l.size(0)
        
        if self.tactile_encoder is not None:
            # ä½¿ç”¨é¢„è®­ç»ƒç¼–ç å™¨æå–ç‰¹å¾
            with torch.no_grad():  # ç¼–ç å™¨å·²å†»ç»“ï¼Œä¸éœ€è¦æ¢¯åº¦
                features_l = self.tactile_encoder.encoder(forces_l)  # (B, feature_dim)
                features_r = self.tactile_encoder.encoder(forces_r)  # (B, feature_dim)
        else:
            # å¦‚æœæ²¡æœ‰ç¼–ç å™¨ï¼Œä½¿ç”¨ç®€å•çš„å…¨å±€å¹³å‡æ± åŒ–ä½œä¸ºç‰¹å¾
            features_l = torch.mean(forces_l.view(batch_size, -1), dim=1, keepdim=True)
            features_r = torch.mean(forces_r.view(batch_size, -1), dim=1, keepdim=True)
            # æ‰©å±•åˆ°æŒ‡å®šçš„ç‰¹å¾ç»´åº¦
            features_l = features_l.repeat(1, self.feature_dim)
            features_r = features_r.repeat(1, self.feature_dim)
        
        # è¿æ¥å·¦å³æ‰‹ç‰¹å¾
        combined_features = torch.cat([features_l, features_r], dim=1)  # (B, feature_dim*2)
        
        # MLPé¢„æµ‹å¤šæ­¥åŠ¨ä½œåºåˆ—
        flat_predictions = self.mlp(combined_features)  # (B, H*3)
        
        # é‡å¡‘ä¸ºåºåˆ—å½¢å¼
        pred_seq = flat_predictions.view(batch_size, self.horizon, self.action_dim)  # (B, H, 3)
        
        return pred_seq


def compute_multistep_losses(predictions, targets, config=None):
    """
    è®¡ç®—å¤šæ­¥é¢„æµ‹æŸå¤±
    
    Args:
        predictions: æ¨¡å‹é¢„æµ‹çš„Hæ­¥åŠ¨ä½œåºåˆ— (B, H, 3)
        targets: çœŸå®Hæ­¥åŠ¨ä½œåºåˆ— (B, H, 3)
        config: æŸå¤±é…ç½®
        
    Returns:
        dict: åŒ…å«å„ç§æŸå¤±çš„å­—å…¸
    """
    if config is None:
        config = {'loss_type': 'mse', 'step_weights': None}
    
    loss_type = config.get('loss_type', 'mse')
    step_weights = config.get('step_weights', None)
    
    batch_size, horizon, action_dim = predictions.shape
    
    # é€æ­¥æŸå¤±è®¡ç®—
    step_losses = []
    
    for h in range(horizon):
        pred_h = predictions[:, h, :]  # (B, 3)
        target_h = targets[:, h, :]    # (B, 3)
        
        if loss_type == 'mse':
            step_loss = F.mse_loss(pred_h, target_h, reduction='mean')
        elif loss_type == 'huber':
            delta = config.get('huber_delta', 1.0)
            step_loss = F.huber_loss(pred_h, target_h, delta=delta, reduction='mean')
        elif loss_type == 'l1':
            step_loss = F.l1_loss(pred_h, target_h, reduction='mean')
        else:
            step_loss = F.mse_loss(pred_h, target_h, reduction='mean')
        
        step_losses.append(step_loss)
    
    # åº”ç”¨æ­¥é•¿æƒé‡ï¼ˆå¦‚æœæä¾›ï¼‰
    if step_weights is not None:
        assert len(step_weights) == horizon, f"æƒé‡é•¿åº¦{len(step_weights)}ä¸horizon{horizon}ä¸åŒ¹é…"
        # åŠ æƒæŸå¤±ï¼šæ¯æ­¥æŸå¤±ä¹˜ä»¥å¯¹åº”æƒé‡
        weighted_losses = [w * loss for w, loss in zip(step_weights, step_losses)]
        total_loss = sum(weighted_losses) / sum(step_weights)  # æ ‡å‡†åŒ–å¤„ç†
    else:
        # é»˜è®¤ç­‰æƒé‡
        total_loss = sum(step_losses) / horizon
    
    # è®¡ç®—æŒ‡æ ‡
    with torch.no_grad():
        # æ€»ä½“MSE
        mse_loss = F.mse_loss(predictions, targets, reduction='mean')
        
        # å„æ­¥MSE
        step_mses = []
        for h in range(horizon):
            step_mse = F.mse_loss(predictions[:, h, :], targets[:, h, :], reduction='mean')
            step_mses.append(step_mse.item())
        
        # æœ€ç»ˆæ­¥æŸå¤±ï¼ˆé€šå¸¸æœ€é‡è¦ï¼‰
        final_step_loss = step_losses[-1].item()
        
        # ç´¯ç§¯è¯¯å·®ï¼ˆæ¯æ­¥è¯¯å·®ç´¯åŠ ï¼‰
        cumulative_error = 0
        for h in range(horizon):
            cumulative_error += F.l1_loss(predictions[:, h, :], targets[:, h, :], reduction='mean')
        cumulative_error = cumulative_error / horizon
    
    return {
        'total_loss': total_loss,
        'mse_loss': mse_loss.item(),
        'step_losses': [loss.item() for loss in step_losses],
        'step_mses': step_mses,
        'final_step_loss': final_step_loss,
        'cumulative_error': cumulative_error.item()
    }


def test_multistep_model():
    """æµ‹è¯•å¤šæ­¥é¢„æµ‹æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•Feature-MLPå¤šæ­¥é¢„æµ‹æ¨¡å‹...")
    
    # åˆ›å»ºæ¨¡å‹
    model = FeatureMLPMultiStep(
        feature_dim=128,
        horizon=5,
        action_dim=3,
        hidden_dims=[256, 128],
        dropout_rate=0.25,
        pretrained_encoder_path=None  # æµ‹è¯•æ—¶ä¸åŠ è½½é¢„è®­ç»ƒæƒé‡
    )
    
    # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    batch_size = 4
    forces_l = torch.randn(batch_size, 3, 20, 20)
    forces_r = torch.randn(batch_size, 3, 20, 20)
    
    print(f"\nğŸ“Š æµ‹è¯•è¾“å…¥:")
    print(f"   å·¦æ‰‹è§¦è§‰: {forces_l.shape}")
    print(f"   å³æ‰‹è§¦è§‰: {forces_r.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        pred_seq = model(forces_l, forces_r)
    
    print(f"\nğŸ“ˆ è¾“å‡º:")
    print(f"   é¢„æµ‹åºåˆ—: {pred_seq.shape}")
    print(f"   é¢„æµ‹èŒƒå›´: [{pred_seq.min().item():.3f}, {pred_seq.max().item():.3f}]")
    
    # æµ‹è¯•æŸå¤±è®¡ç®—
    target_seq = torch.randn_like(pred_seq)
    losses = compute_multistep_losses(pred_seq, target_seq)
    
    print(f"\nğŸ“‰ æŸå¤±æµ‹è¯•:")
    print(f"   æ€»æŸå¤±: {losses['total_loss']:.4f}")
    print(f"   MSEæŸå¤±: {losses['mse_loss']:.4f}")
    print(f"   æœ€ç»ˆæ­¥æŸå¤±: {losses['final_step_loss']:.4f}")
    print(f"   å„æ­¥æŸå¤±: {[f'{x:.4f}' for x in losses['step_losses']]}")
    
    print("\nâœ… å¤šæ­¥é¢„æµ‹æ¨¡å‹æµ‹è¯•å®Œæˆ!")


if __name__ == '__main__':
    test_multistep_model()
