# Improved Prototype STN Autoencoder ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

`ImprovedPrototypeSTNAE` æ˜¯åŸºäºåŸç‰ˆ `PrototypeAutoencoder` çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œç»“åˆäº†ï¼š

1. **æ”¹è¿›çš„ç½‘ç»œæ¶æ„**: æ›´æ·±çš„CNNç¼–ç å™¨ï¼ŒBatchNormå’ŒDropoutæ­£åˆ™åŒ–
2. **STNç©ºé—´å˜æ¢**: æ”¯æŒå…±äº«å’Œç‹¬ç«‹ä¸¤ç§STNæ¨¡å¼
3. **Xavieråˆå§‹åŒ–**: æ”¹è¿›çš„åŸå‹å’Œæƒé‡åˆå§‹åŒ–ç­–ç•¥
4. **éšæœºæ‰°åŠ¨**: è®­ç»ƒæ—¶æ·»åŠ å™ªå£°é˜²æ­¢è¿‡æ‹Ÿåˆ
5. **æ”¹è¿›æŸå¤±å‡½æ•°**: åŒ…å«STNç‰¹æœ‰çš„æ­£åˆ™åŒ–é¡¹

## ğŸš€ å¿«é€Ÿå¼€å§‹

```python
import torch
from models.improved_prototype_ae_STN import (
    ImprovedPrototypeSTNAE, 
    compute_improved_stn_losses
)

# åˆ›å»ºæ¨¡å‹
model = ImprovedPrototypeSTNAE(
    num_prototypes=8,           # åŸå‹æ•°é‡
    input_shape=(3, 20, 20),    # è¾“å…¥å½¢çŠ¶ (C, H, W)
    share_stn=True              # æ˜¯å¦å…±äº«STNå‚æ•°
)

# è®­ç»ƒ
model.train()
x = torch.randn(4, 3, 20, 20)  # æ‰¹æ¬¡æ•°æ®
recon, weights, transformed_protos, thetas = model(x)

# è®¡ç®—æŸå¤±
total_loss, loss_dict = compute_improved_stn_losses(
    x, recon, weights, transformed_protos, thetas,
    diversity_lambda=1.0,    # å¤šæ ·æ€§æŸå¤±æƒé‡
    entropy_lambda=0.1,      # ç†µæŸå¤±æƒé‡
    sparsity_lambda=0.01,    # ç¨€ç–æ€§æŸå¤±æƒé‡
    stn_reg_lambda=0.05      # STNæ­£åˆ™åŒ–æƒé‡
)

# åå‘ä¼ æ’­
total_loss.backward()
```

## ğŸ—ï¸ æ¨¡å‹æ¶æ„ç‰¹ç‚¹

### 1. æ”¹è¿›çš„åŸå‹åˆå§‹åŒ–
```python
# ä½¿ç”¨Xavieråˆå§‹åŒ–å¹¶æ·»åŠ å°çš„éšæœºåç§»
self.prototypes = nn.Parameter(torch.zeros(num_prototypes, C, H, W))
nn.init.xavier_normal_(self.prototypes, gain=0.1)
```

### 2. å¢å¼ºçš„CNNç¼–ç å™¨
- **3å±‚å·ç§¯**: 32â†’64â†’128é€šé“
- **BatchNorm**: æ¯å±‚éƒ½æœ‰æ‰¹å½’ä¸€åŒ–
- **Dropout**: 2Då’Œ1D Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
- **Softmaxè¾“å‡º**: ç¡®ä¿æƒé‡å½’ä¸€åŒ–

### 3. æ”¹è¿›çš„STNæ¨¡å—
- **æ›´æ·±ç½‘ç»œ**: å¢åŠ äº†BatchNormå’ŒDropout
- **éšæœºæ‰°åŠ¨**: è®­ç»ƒæ—¶æ·»åŠ å™ªå£°é˜²æ­¢è¿‡æ‹Ÿåˆ
- **å…±äº«æ¨¡å¼**: æ”¯æŒå¤šåŸå‹å…±äº«STNå‚æ•°

### 4. éšæœºæ‰°åŠ¨æœºåˆ¶
```python
# æƒé‡æ‰°åŠ¨
if self.training:
    noise = torch.randn_like(weights) * 0.01
    weights = weights + noise
    weights = F.softmax(weights, dim=-1)

# STNå‚æ•°æ‰°åŠ¨
if self.training:
    noise = torch.randn_like(theta) * 0.01
    theta = theta + noise
```

## ğŸ“Š æŸå¤±å‡½æ•°è¯¦è§£

### 1. é‡æ„æŸå¤± (Huber Loss)
```python
recon_loss = F.smooth_l1_loss(recon, x)
```
- å¯¹å¼‚å¸¸å€¼æ›´é²æ£’
- æ›¿ä»£MSEæŸå¤±

### 2. å¤šæ ·æ€§æŸå¤±
```python
# æƒ©ç½šåŸå‹é—´é«˜ç›¸ä¼¼åº¦
sim_matrix = torch.matmul(protos_norm, protos_norm.T)
diversity_loss = torch.clamp(off_diag_sim, min=0).pow(2).mean()
```

### 3. æ”¹è¿›çš„ç†µæŸå¤±
```python
# ä½¿ç”¨KLæ•£åº¦é¼“åŠ±é€‚åº¦çš„æƒé‡åˆ†å¸ƒ
uniform_dist = torch.ones_like(weights) / K
entropy_loss = F.kl_div(torch.log(weights + 1e-8), uniform_dist, reduction='batchmean')
```

### 4. ç¨€ç–æ€§æŸå¤± (åŸºå°¼ç³»æ•°)
```python
def gini_coefficient(w):
    # è®¡ç®—åŸºå°¼ç³»æ•°è¡¡é‡åˆ†å¸ƒä¸å‡åŒ€ç¨‹åº¦
    sorted_w, _ = torch.sort(w, dim=1, descending=False)
    n = w.size(1)
    index = torch.arange(1, n + 1, dtype=torch.float32, device=w.device)
    return ((2 * index - n - 1) * sorted_w).sum(dim=1) / (n * sorted_w.sum(dim=1) + 1e-8)

sparsity_loss = (1.0 - gini_coefficient(weights)).mean()
```

### 5. STNæ­£åˆ™åŒ–æŸå¤±
```python
# åˆ†åˆ«æƒ©ç½šæ—‹è½¬/ç¼©æ”¾å’Œå¹³ç§»
rotation_scale_loss = F.mse_loss(theta_diff[:, :, :, :2], ...)
translation_loss = F.mse_loss(theta_diff[:, :, :, 2], ...)
stn_loss = rotation_scale_loss + 0.5 * translation_loss

# STNå¤šæ ·æ€§: é¼“åŠ±ä¸åŒåŸå‹å­¦ä¹ ä¸åŒå˜æ¢
theta_diversity_loss = ...
```

## ğŸ“ˆ æ¨¡å‹å¯¹æ¯”

| ç‰¹æ€§ | åŸç‰ˆæ¨¡å‹ | æ”¹è¿›ç‰ˆæ¨¡å‹ |
|------|----------|------------|
| å‚æ•°é‡ | ~44K | ~235K |
| åŸå‹åˆå§‹åŒ– | éšæœº | Xavier |
| CNNæ·±åº¦ | 2å±‚ | 3å±‚ |
| æ­£åˆ™åŒ– | æ—  | BatchNorm + Dropout |
| æŸå¤±å‡½æ•° | MSE | Huber + åŸºå°¼ç³»æ•° |
| STNæ­£åˆ™åŒ– | åŸºç¡€ | å¢å¼ºç‰ˆ |
| éšæœºæ‰°åŠ¨ | æ—  | æœ‰ |

## ğŸ¯ ä½¿ç”¨å»ºè®®

### 1. å‚æ•°é€‰æ‹©
- **num_prototypes**: æ ¹æ®æ•°æ®å¤æ‚åº¦é€‰æ‹©ï¼Œæ¨è8-16
- **share_stn**: å…±äº«STNå‚æ•°é‡å°‘ï¼Œç‹¬ç«‹STNè¡¨è¾¾èƒ½åŠ›å¼º
- **æŸå¤±æƒé‡**: å¯æ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´

### 2. è®­ç»ƒæŠ€å·§
```python
# æ¨èçš„è®­ç»ƒé…ç½®
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.8)

# æŸå¤±æƒé‡æ¨èå€¼
loss_config = {
    'diversity_lambda': 1.0,
    'entropy_lambda': 0.1,
    'sparsity_lambda': 0.01,
    'stn_reg_lambda': 0.05
}
```

### 3. å…¼å®¹æ€§
- å®Œå…¨å…¼å®¹åŸç‰ˆAPI
- å¯ç›´æ¥æ›¿æ¢åŸç‰ˆæ¨¡å‹
- æ”¯æŒç›¸åŒçš„è¾“å…¥è¾“å‡ºæ ¼å¼

## ğŸ”§ è°ƒè¯•ä¸ç›‘æ§

æ¨¡å‹è¿”å›è¯¦ç»†çš„æŸå¤±ä¿¡æ¯ï¼š
```python
{
    "recon_loss": 0.42,           # é‡æ„æŸå¤±
    "diversity_loss": 0.001,      # å¤šæ ·æ€§æŸå¤±
    "entropy_loss": 0.002,        # ç†µæŸå¤±
    "sparsity_loss": 0.97,        # ç¨€ç–æ€§æŸå¤± (åŸºäºåŸºå°¼ç³»æ•°)
    "gini_coeff": 0.03,           # åŸºå°¼ç³»æ•° (è¶Šå¤§è¶Šç¨€ç–)
    "stn_loss": 1.90,             # STNæ­£åˆ™åŒ–æŸå¤±
    "theta_diversity_loss": 0.04, # STNå¤šæ ·æ€§æŸå¤±
    "total_loss": 0.53            # æ€»æŸå¤±
}
```

## ğŸ“‹ æµ‹è¯•ç»“æœ

âœ… **åŠŸèƒ½æµ‹è¯•**: æ‰€æœ‰åŸºç¡€åŠŸèƒ½æ­£å¸¸
âœ… **å…¼å®¹æ€§æµ‹è¯•**: ä¸åŸç‰ˆAPIå®Œå…¨å…¼å®¹
âœ… **æ€§èƒ½æµ‹è¯•**: GPUåŠ é€Ÿæ­£å¸¸ï¼Œå†…å­˜ä½¿ç”¨åˆç†
âœ… **æ•°å€¼ç¨³å®šæ€§**: æƒé‡å½’ä¸€åŒ–æ­£ç¡®ï¼Œæ¢¯åº¦ç¨³å®š

---

*è¿™ä¸ªæ”¹è¿›ç‰ˆæœ¬åœ¨ä¿æŒåŸæœ‰åŠŸèƒ½çš„åŸºç¡€ä¸Šï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œè®­ç»ƒç¨³å®šæ€§ï¼*
