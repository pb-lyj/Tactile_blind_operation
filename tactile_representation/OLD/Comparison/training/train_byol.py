"""
BYOLè®­ç»ƒè„šæœ¬
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from datetime import datetime
import argparse
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "..")))

from tactile_representation.Prototype_Discovery.dataset_dataloader.tactile_dataset import TactileForcesDataset
from tactile_representation.models.byol_model import create_tactile_byol, create_byol_loss, create_data_augmentation, TactileBYOLClassifier
from utils.logging import Logger
from utils.visualization import plot_loss_curves
from utils.data_utils import save_physicalXYZ_images, save_sample_weights_and_analysis


class BYOLDataset(torch.utils.data.Dataset):
    """åŒ…è£…æ•°æ®é›†ä»¥åº”ç”¨BYOLå¢å¼º"""
    
    def __init__(self, base_dataset, augmentation):
        self.base_dataset = base_dataset
        self.augmentation = augmentation
    
    def __len__(self):
        return len(self.base_dataset)
    
    def __getitem__(self, idx):
        batch = self.base_dataset[idx]
        image = batch['image']
        
        # åº”ç”¨BYOLå¢å¼º
        aug1, aug2 = self.augmentation(image)
        
        return {
            'image1': aug1,
            'image2': aug2,
            'original': image
        }


class BYOLTrainer:
    """BYOLè®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = os.path.join(config['output_dir'], f"byol_{timestamp}")
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        with open(os.path.join(self.output_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=2)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = create_tactile_byol(config['model']).to(self.device)
        self.criterion = create_byol_loss()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=config['training']['learning_rate'],
            weight_decay=config['training']['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=config['training']['epochs']
        )
        
        # è®­ç»ƒå†å²
        self.train_losses = []
        self.val_losses = []
        
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")

    def train_epoch(self, dataloader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        
        pbar = tqdm(dataloader, desc="Training")
        for batch_idx, batch in enumerate(pbar):
            image1 = batch['image1'].to(self.device)
            image2 = batch['image2'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(image1, image2)
            loss_dict = self.criterion(outputs)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss_dict['total_loss'].backward()
            self.optimizer.step()
            
            # æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self.model.update_target_network()
            
            # ç»Ÿè®¡
            total_loss += loss_dict['total_loss'].item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f"{loss_dict['total_loss'].item():.4f}",
                'LR': f"{self.optimizer.param_groups[0]['lr']:.6f}"
            })
        
        return {
            'total_loss': total_loss / len(dataloader),
            'byol_loss': total_loss / len(dataloader)
        }

    def validate(self, dataloader):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Validation"):
                image1 = batch['image1'].to(self.device)
                image2 = batch['image2'].to(self.device)
                
                outputs = self.model(image1, image2)
                loss_dict = self.criterion(outputs)
                
                total_loss += loss_dict['total_loss'].item()
        
        return {
            'total_loss': total_loss / len(dataloader),
            'byol_loss': total_loss / len(dataloader)
        }

    def train(self, train_loader, val_loader):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        best_val_loss = float('inf')
        
        for epoch in range(self.config['training']['epochs']):
            print(f"\nEpoch {epoch+1}/{self.config['training']['epochs']}")
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader)
            self.train_losses.append(train_metrics)
            
            # éªŒè¯
            val_metrics = self.validate(val_loader)
            self.val_losses.append(val_metrics)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # æ‰“å°ç»“æœ
            print(f"Train Loss: {train_metrics['total_loss']:.4f}, "
                  f"Val Loss: {val_metrics['total_loss']:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['total_loss'] < best_val_loss:
                best_val_loss = val_metrics['total_loss']
                self.save_model('best_model.pth')
            
            # å®šæœŸä¿å­˜
            if (epoch + 1) % self.config['training']['save_freq'] == 0:
                self.save_model(f'model_epoch_{epoch+1}.pth')
                self.plot_losses()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model('final_model.pth')
        self.plot_losses()
        
        # è¿”å›æ¨¡å‹å’ŒæŸå¤±å†å²
        loss_history = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses
        }
        return self.model, loss_history

    def train_single_dataset(self, data_dir, epochs=50, early_stopping_patience=5):
        """
        ä¸ºå•ä¸€æ•°æ®é›†è®­ç»ƒBYOLæ¨¡å‹ - å…¼å®¹Prototype Discoveryæ ¼å¼
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            epochs: è®­ç»ƒè½®æ•°
            early_stopping_patience: æ—©åœè€å¿ƒå€¼
        
        Returns:
            loss_history: ä¸Prototype Discoveryæ ¼å¼ä¸€è‡´çš„æŸå¤±å†å²
        """
        print(f"ğŸš€ å¼€å§‹BYOLè®­ç»ƒå•ä¸€æ•°æ®é›†: {data_dir}")
        print(f"è®­ç»ƒè½®æ•°: {epochs}")
        
        # åˆ›å»ºæ•°æ®é›†
        full_dataset = TactileForcesDataset(
            data_root=data_dir,
            categories=None,  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨ç±»åˆ«
            start_frame=0,
            exclude_test_folders=True
        )
        
        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›† (80/20)
        val_size = int(len(full_dataset) * 0.2)
        train_size = len(full_dataset) - val_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size]
        )
        
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {train_size}, éªŒè¯æ ·æœ¬æ•°: {val_size}")
        
        # åˆ›å»ºæ•°æ®å¢å¼º
        augmentation = create_data_augmentation()
        train_byol_dataset = BYOLDataset(train_dataset, augmentation)
        val_byol_dataset = BYOLDataset(val_dataset, augmentation)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_byol_dataset,
            batch_size=self.config['training']['batch_size'],
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_byol_dataset,
            batch_size=self.config['training']['batch_size'],
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        # åˆå§‹åŒ–æŸå¤±å†å² - ä¸Prototype Discoveryæ ¼å¼ä¸€è‡´
        loss_history = {
            'total_loss': [],
            'byol_loss': [],
            'recon_loss': []  # BYOLæ²¡æœ‰é‡å»ºæŸå¤±ï¼Œä½†ä¸ºä¿æŒä¸€è‡´æ€§è®¾ä¸ºä¸byol_lossç›¸åŒ
        }
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        print("\n" + "="*60)
        print("å¼€å§‹è®­ç»ƒBYOLæ¨¡å‹")
        print("="*60)
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            epoch_losses = {'total_loss': 0, 'byol_loss': 0}
            num_batches = 0
            
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
            for batch in pbar:
                image1 = batch['image1'].to(self.device)
                image2 = batch['image2'].to(self.device)
                
                self.optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                z1, z2, p1, p2 = self.model(image1, image2)
                loss = self.criterion(p1, z2) + self.criterion(p2, z1)
                
                # åå‘ä¼ æ’­
                loss.backward()
                self.optimizer.step()
                
                # æ›´æ–°EMAæƒé‡
                self.model.update_target_network()
                
                # è®°å½•æŸå¤±
                epoch_losses['total_loss'] += loss.item()
                epoch_losses['byol_loss'] += loss.item()
                num_batches += 1
                
                pbar.set_postfix({'Loss': f"{loss.item():.4f}"})
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_train_loss = epoch_losses['total_loss'] / num_batches
            avg_byol_loss = epoch_losses['byol_loss'] / num_batches
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_losses = {'total_loss': 0, 'byol_loss': 0}
            val_batches = 0
            
            with torch.no_grad():
                for batch in val_loader:
                    image1 = batch['image1'].to(self.device)
                    image2 = batch['image2'].to(self.device)
                    
                    z1, z2, p1, p2 = self.model(image1, image2)
                    loss = self.criterion(p1, z2) + self.criterion(p2, z1)
                    
                    val_losses['total_loss'] += loss.item()
                    val_losses['byol_loss'] += loss.item()
                    val_batches += 1
            
            avg_val_loss = val_losses['total_loss'] / val_batches
            avg_val_byol_loss = val_losses['byol_loss'] / val_batches
            
            # è®°å½•æŸå¤±å†å²
            loss_history['total_loss'].append(avg_train_loss)
            loss_history['byol_loss'].append(avg_byol_loss)
            loss_history['recon_loss'].append(avg_byol_loss)  # ä¸byol_lossç›¸åŒ
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # æ‰“å°è®­ç»ƒè¿›åº¦
            print(f"Epoch [{epoch+1}/{epochs}] - "
                  f"Train Loss: {avg_train_loss:.4f}, "
                  f"Val Loss: {avg_val_loss:.4f}, "
                  f"LR: {self.scheduler.get_last_lr()[0]:.6f}")
            
            # æ—©åœæ£€æŸ¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.save_model('best_byol_model.pth')
            else:
                patience_counter += 1
                if patience_counter >= early_stopping_patience:
                    print(f"\nâš ï¸ æ—©åœè§¦å‘ï¼éªŒè¯æŸå¤±è¿ç»­{early_stopping_patience}è½®æœªæ”¹å–„")
                    break
        
        print(f"\nâœ… BYOLè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        
        return loss_history

    def save_model(self, filename):
        """ä¿å­˜æ¨¡å‹"""
        filepath = os.path.join(self.output_dir, filename)
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'config': self.config
        }, filepath)
        print(f"æ¨¡å‹å·²ä¿å­˜: {filepath}")

    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.train_losses = checkpoint['train_losses']
        self.val_losses = checkpoint['val_losses']
        print(f"æ¨¡å‹å·²åŠ è½½: {filepath}")

    def plot_losses(self):
        """ç»˜åˆ¶æŸå¤±æ›²çº¿"""
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        
        epochs = range(1, len(self.train_losses) + 1)
        
        # BYOLæŸå¤±
        train_total = [m['total_loss'] for m in self.train_losses]
        val_total = [m['total_loss'] for m in self.val_losses]
        ax.plot(epochs, train_total, label='Train')
        ax.plot(epochs, val_total, label='Validation')
        ax.set_title('BYOL Loss')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'training_curves.png'))
        plt.close()

    def visualize_features(self, dataloader, num_samples=8):
        """å¯è§†åŒ–å­¦ä¹ åˆ°çš„ç‰¹å¾"""
        self.model.eval()
        
        features = []
        images = []
        
        with torch.no_grad():
            for batch in dataloader:
                original = batch['original'][:num_samples].to(self.device)
                feature = self.model.encode(original)
                
                features.append(feature.cpu().numpy())
                images.append(original.cpu().numpy())
                
                if len(features) * num_samples >= 64:  # æ”¶é›†è¶³å¤Ÿçš„æ ·æœ¬
                    break
        
        features = np.vstack(features)
        images = np.vstack(images)
        
        # ä½¿ç”¨PCAé™ç»´å¯è§†åŒ–ç‰¹å¾
        try:
            from sklearn.decomposition import PCA
            from sklearn.manifold import TSNE
            
            # PCAé™ç»´
            pca = PCA(n_components=2)
            features_2d = pca.fit_transform(features)
            
            # ç»˜åˆ¶ç‰¹å¾åˆ†å¸ƒ
            plt.figure(figsize=(10, 8))
            scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], 
                                c=range(len(features_2d)), cmap='viridis', alpha=0.7)
            plt.colorbar(scatter)
            plt.title('BYOL Feature Visualization (PCA)')
            plt.xlabel('PC1')
            plt.ylabel('PC2')
            plt.savefig(os.path.join(self.output_dir, 'feature_visualization.png'))
            plt.close()
            
        except ImportError:
            print("sklearnæœªå®‰è£…ï¼Œè·³è¿‡ç‰¹å¾å¯è§†åŒ–")

    def create_augmented_datasets(self, base_train_dataset, base_val_dataset):
        """åˆ›å»ºåŒ…å«å¢å¼ºçš„æ•°æ®é›†"""
        augmentation = create_data_augmentation(self.config.get('augmentation', {}))
        
        train_dataset = BYOLDataset(base_train_dataset, augmentation)
        val_dataset = BYOLDataset(base_val_dataset, augmentation)
        
        return train_dataset, val_dataset


def get_default_config():
    """è·å–é»˜è®¤é…ç½®"""
    return {
        'model': {
            'projection_dim': 128,
            'prediction_dim': 128,
            'hidden_dim': 256,
            'moving_average_decay': 0.99
        },
        'augmentation': {
            'noise_std': 0.1,
            'rotation_angle': 15,
            'flip_prob': 0.5
        },
        'data': {
            'data_root': './data/data25.7_aligned',
            'categories': None,
            'batch_size': 64,
            'val_split': 0.2,
            'num_workers': 4
        },
        'training': {
            'epochs': 300,
            'learning_rate': 1e-3,
            'weight_decay': 1e-4,
            'save_freq': 25
        },
        'output_dir': './outputs/byol'
    }


def main(config):
    """
    ä¸»è®­ç»ƒå‡½æ•° - æ”¯æŒå¤šæ•°æ®é›†å’Œå•æ•°æ®é›†è®­ç»ƒ
    """
    # è®¾ç½®æ—¥å¿—
    output_dir = config['output']['output_dir']
    os.makedirs(output_dir, exist_ok=True)
    log_file = os.path.join(output_dir, "train_byol.log")
    sys.stdout = Logger(log_file)
    
    print("=" * 60)
    print("BYOL Tactile Representation Training")
    print(f"Output Directory: {output_dir}")
    print(f"Data Root: {config['data']['data_root']}")
    print(f"Batch Size: {config['data']['batch_size']}")
    print(f"Epochs: {config['training']['epochs']}")
    print(f"Learning Rate: {config['training']['learning_rate']}")
    print("=" * 60)
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = TactileForcesDataset(
        data_root=config['data']['data_root'],
        categories=config['data']['categories'] or [
            "cir_lar", "cir_med", "cir_sma",
            "rect_lar", "rect_med", "rect_sma", 
            "tri_lar", "tri_med", "tri_sma"
        ],
        start_frame=0,
        exclude_test_folders=True
    )
    
    # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
    val_size = int(len(dataset) * config['data']['val_split'])
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = BYOLTrainer(config)
    
    # åˆ›å»ºåŒ…å«å¢å¼ºçš„æ•°æ®é›†
    train_dataset_aug, val_dataset_aug = trainer.create_augmented_datasets(
        train_dataset, val_dataset
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset_aug,
        batch_size=config['data']['batch_size'],
        shuffle=True,
        num_workers=config['data']['num_workers'],
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset_aug,
        batch_size=config['data']['batch_size'],
        shuffle=False,
        num_workers=config['data']['num_workers'],
        pin_memory=True
    )
    
    # å¼€å§‹è®­ç»ƒ
    model, loss_history = trainer.train(train_loader, val_loader)
    
    # ä¿å­˜è®­ç»ƒæŸå¤±æ›²çº¿ - æ‰€æœ‰æŸå¤±åœ¨ä¸€å¼ å›¾ä¸Š
    from utils.visualization import plot_all_losses_single_plot
    plot_all_losses_single_plot(
        loss_history, 
        save_path=os.path.join(output_dir, "training_loss_curves.png"),
        title="BYOL Training Loss"
    )
    
    # ä¿å­˜æŸå¤±å†å²æ•°æ®
    np.save(os.path.join(output_dir, "loss_history.npy"), loss_history)
    
    # å¯è§†åŒ–ç‰¹å¾
    trainer.visualize_features(val_loader)
    
    print("âœ… BYOLæ¨¡å‹è®­ç»ƒå®Œæˆ!")
    return model, loss_history


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='BYOLè®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_root', type=str, help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--epochs', type=int, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, help='å­¦ä¹ ç‡')
    
    args = parser.parse_args()
    
    # é»˜è®¤é…ç½®
    config = {
        'data': {
            'data_root': '/home/lyj/Program_python/Tactile_blind_operation/datasets/data25.7_aligned',
            'categories': None,
            'batch_size': 64,
            'val_split': 0.2,
            'num_workers': 4
        },
        'model': {
            'image_size': (20, 20),
            'backbone': 'resnet18',
            'projection_dim': 128,
            'hidden_dim': 512,
            'temperature': 0.2,
            'tau': 0.996
        },
        'training': {
            'epochs': 300,
            'learning_rate': 1e-3,
            'weight_decay': 1e-4,
            'save_freq': 25,
            'patience': 50
        },
        'output': {
            'output_dir': os.path.join("./cluster/comparison_library", 
                                     datetime.now().strftime("%Y-%m-%d_%H-%M-%S") + "_byol")
        }
    }
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½
    if args.config:
        with open(args.config, 'r') as f:
            loaded_config = json.load(f)
        config.update(loaded_config)
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.data_root:
        config['data']['data_root'] = args.data_root
    if args.epochs:
        config['training']['epochs'] = args.epochs
    if args.batch_size:
        config['data']['batch_size'] = args.batch_size
    if args.lr:
        config['training']['learning_rate'] = args.lr
    
    main(config)
