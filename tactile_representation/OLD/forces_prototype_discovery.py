import os
import sys
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(PROJECT_ROOT)

from utils.logging import Logger
from utils.visualization import save_physicalXYZ_images, plot_loss_curves
from utils.data_utils import save_sample_weights_and_analysis
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from glob import glob
from tqdm import tqdm
from sklearn.manifold import TSNE
from models.prototype_ae_STN import PrototypeAutoencoder, compute_losses
from models.prototype_ae_baseline import PrototypeAEBaseline, compute_baseline_losses
from models.improved_prototype_ae import ImprovedForcePrototypeAE, compute_improved_losses
from models.improved_prototype_ae_STN import ImprovedPrototypeSTNAE, compute_improved_stn_losses

# ==================== Config ====================
# æ–°çš„æ•°æ®æ ¹ç›®å½• - data25.7_alignedä¸­çš„å„ç±»åˆ«
DATA_CATEGORIES = [
    "cir_lar", "cir_med", "cir_sma",
    "rect_lar", "rect_med", "rect_sma", 
    "tri_lar", "tri_med", "tri_sma"
]
DATA_ROOT = "./data/data25.7_aligned"
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
OUTPUT_DIR = os.path.join("./cluster/prototype_library", timestamp)
NUM_PROTOTYPES = 8  # åŸå‹æ•°é‡
BATCH_SIZE = 64     # å‡å°æ‰¹æ¬¡å¤§å°ï¼Œæé«˜æ¢¯åº¦ç¨³å®šæ€§
EPOCHS = 50        # å¢åŠ è®­ç»ƒè½®æ•°
LR = 1e-4          # é™ä½å­¦ä¹ ç‡
START_FRAME = 0    # ä»ç¬¬å‡ å¸§å¼€å§‹æˆªå–

# ==================== Dataset ====================

class TactileForcesDataset(Dataset):
    def __init__(self, data_root, categories=None, start_frame=START_FRAME, exclude_test_folders=True):
        """
        Args:
            data_root: æ•°æ®æ ¹ç›®å½•è·¯å¾„ (data25.7_aligned)
            categories: è¦åŒ…å«çš„ç±»åˆ«åˆ—è¡¨ï¼Œå¦‚ ["cir_lar", "rect_med"] ç­‰
            start_frame: ä»ç¬¬å‡ å¸§å¼€å§‹æˆªå–æ•°æ®
            exclude_test_folders: æ˜¯å¦æ’é™¤æµ‹è¯•æ–‡ä»¶å¤¹(ç¬¬10,20,30,40,50ä¸ª)
        """
        self.samples = []
        if categories is None:
            categories = DATA_CATEGORIES
            
        # å®šä¹‰è¦æ’é™¤çš„æµ‹è¯•æ–‡ä»¶å¤¹ç´¢å¼• (1-based)
        test_folder_indices = {10, 20, 30, 40, 50} if exclude_test_folders else set()
        
        total_frames = 0
        valid_frames = 0
        total_episodes = 0
        excluded_episodes = 0
        
        for category in categories:
            category_path = os.path.join(data_root, category)
            if not os.path.exists(category_path):
                print(f"è­¦å‘Š: ç±»åˆ«è·¯å¾„ä¸å­˜åœ¨: {category_path}")
                continue
                
            # è·å–è¯¥ç±»åˆ«ä¸‹çš„æ‰€æœ‰æ ·æœ¬ç›®å½•ï¼ŒæŒ‰åç§°æ’åºç¡®ä¿é¡ºåºä¸€è‡´
            sample_dirs = sorted(glob(os.path.join(category_path, "2025*")))
            
            for idx, sample_dir in enumerate(sample_dirs, 1):  # 1-basedç´¢å¼•
                total_episodes += 1
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ–‡ä»¶å¤¹
                if idx in test_folder_indices:
                    excluded_episodes += 1
                    # print(f"æ’é™¤æµ‹è¯•æ–‡ä»¶å¤¹: {category} ç¬¬{idx}ä¸ª - {os.path.basename(sample_dir)}")
                    continue
                
                try:
                    # åŠ è½½å·¦å³æ‰‹çš„åŠ›æ•°æ®
                    forces_l_path = os.path.join(sample_dir, "_forces_l.npy")
                    forces_r_path = os.path.join(sample_dir, "_forces_r.npy")
                    
                    if not (os.path.exists(forces_l_path) and os.path.exists(forces_r_path)):
                        continue
                        
                    forces_l = np.load(forces_l_path)  # shape (T, 3, 20, 20)
                    forces_r = np.load(forces_r_path)  # shape (T, 3, 20, 20)
                    
                    # ç¡®ä¿å·¦å³æ‰‹æ•°æ®é•¿åº¦ä¸€è‡´
                    min_length = min(forces_l.shape[0], forces_r.shape[0])
                    forces_l = forces_l[:min_length]
                    forces_r = forces_r[:min_length]
                    
                    total_frames += min_length
                    
                    # åªä½¿ç”¨ä» start_frame å¼€å§‹çš„æ•°æ®
                    if min_length > start_frame:
                        for t in range(start_frame, min_length):
                            # å–æ¯ä¸ªæ—¶é—´æ­¥çš„æ•°æ®
                            frame_l = forces_l[t]  # shape (3, 20, 20) - å·¦æ‰‹ä¼ æ„Ÿå™¨
                            frame_r = forces_r[t]  # shape (3, 20, 20) - å³æ‰‹ä¼ æ„Ÿå™¨
                            
                            self.samples.append(frame_l)  # å·¦æ‰‹æ•°æ®
                            self.samples.append(frame_r)  # å³æ‰‹æ•°æ®
                            valid_frames += 1
                            
                except Exception as e:
                    print(f"å¤„ç†æ ·æœ¬ {sample_dir} æ—¶å‡ºé”™: {e}")
                    continue
            print(f"å…±æ’é™¤ {category}ç±» æµ‹è¯•æ–‡ä»¶å¤¹ {excluded_episodes}ä¸ª")
                    
        if len(self.samples) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®æ ·æœ¬ï¼")
            
        self.samples = np.stack(self.samples)
        
        # æ•°æ®å½’ä¸€åŒ– - æ ¹æ®å®é™…çš„åŠ›æ•°æ®èŒƒå›´è¿›è¡Œè°ƒæ•´
        # è¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…æ•°æ®åˆ†å¸ƒè°ƒæ•´å½’ä¸€åŒ–ç­–ç•¥
        # æš‚æ—¶ä½¿ç”¨ç®€å•çš„æ ‡å‡†åŒ–
        self.samples = self._normalize_data(self.samples)
        
        print(f"[TactileForcesDataset] æ•°æ®ç»Ÿè®¡:")
        print(f"  - æ•°æ®æ ¹ç›®å½•: {data_root}")
        print(f"  - åŒ…å«ç±»åˆ«: {categories}")
        print(f"  - æ€»æƒ…èŠ‚æ•°: {total_episodes}")
        if exclude_test_folders:
            print(f"  - æ’é™¤æµ‹è¯•æƒ…èŠ‚æ•°: {excluded_episodes} (ç¬¬10,20,30,40,50ä¸ªæ–‡ä»¶å¤¹)")
            print(f"  - è®­ç»ƒæƒ…èŠ‚æ•°: {total_episodes - excluded_episodes}")
        print(f"  - æ€»å¸§æ•°: {total_frames}")
        print(f"  - æˆªå–èµ·å§‹å¸§: {start_frame}")
        print(f"  - æœ‰æ•ˆå¸§æ•°: {valid_frames}")
        print(f"  - æ€»æ ·æœ¬æ•°: {len(self.samples)} (åŒ…å«å·¦å³æ‰‹ä¼ æ„Ÿå™¨)")
        print(f"  - æ ·æœ¬å½¢çŠ¶: {self.samples.shape}")
        print(f"  - æ•°æ®èŒƒå›´: [{self.samples.min():.4f}, {self.samples.max():.4f}]")

    def _normalize_data(self, data):
        """
        æ”¹è¿›çš„æ•°æ®å½’ä¸€åŒ–å¤„ç†
        Args:
            data: åŸå§‹æ•°æ® (N, 3, 20, 20)
        Returns:
            æ ‡å‡†åŒ–åçš„æ•°æ®
        """
        print(f"åŸå§‹æ•°æ®èŒƒå›´: [{data.min():.4f}, {data.max():.4f}]")
        
        # æ–¹æ³•3: åˆ†ä½æ•°å½’ä¸€åŒ– + Z-scoreæ ‡å‡†åŒ–ï¼ˆæ¨èï¼‰
        normalized_data = np.zeros_like(data)
        
        for i in range(data.shape[1]):  # å¯¹æ¯ä¸ªé€šé“åˆ†åˆ«å¤„ç†
            channel_data = data[:, i]
            
            # 1. ä½¿ç”¨åˆ†ä½æ•°å»é™¤æç«¯å¼‚å¸¸å€¼
            q1, q99 = np.percentile(channel_data, [1, 99])
            channel_data = np.clip(channel_data, q1, q99)
            
            # 2. Z-scoreæ ‡å‡†åŒ–
            mean = channel_data.mean()
            std = channel_data.std()
            if std > 1e-8:
                normalized_data[:, i] = (channel_data - mean) / std
            else:
                normalized_data[:, i] = channel_data - mean
                
        print(f"æ ‡å‡†åŒ–åæ•°æ®èŒƒå›´: [{normalized_data.min():.4f}, {normalized_data.max():.4f}]")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
        if np.isnan(normalized_data).any() or np.isinf(normalized_data).any():
            print("è­¦å‘Š: æ ‡å‡†åŒ–åæ•°æ®åŒ…å«NaNæˆ–Infå€¼")
            normalized_data = np.nan_to_num(normalized_data, nan=0.0, posinf=1.0, neginf=-1.0)
        
        return normalized_data

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return torch.tensor(self.samples[idx], dtype=torch.float32)

# ==================== Training ====================

def get_model_config(model_type="ImprovedPrototypeSTNAE"):
    """
    è·å–ä¸åŒæ¨¡å‹çš„é…ç½®ä¿¡æ¯
    Args:
        model_type: æ¨¡å‹ç±»å‹ ["PrototypeAutoencoder", "PrototypeAEBaseline", 
                    "ImprovedForcePrototypeAE", "ImprovedPrototypeSTNAE"]
    Returns:
        dict: åŒ…å«æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€æŸå¤±å­—æ®µç­‰é…ç½®
    """
    if model_type == "PrototypeAutoencoder":
        from models.prototype_ae_STN import PrototypeAutoencoder, compute_losses
        model = PrototypeAutoencoder(NUM_PROTOTYPES, input_shape=(3, 20, 20), share_stn=True).cuda()
        compute_loss_fn = compute_losses
        loss_fields = ['recon_loss', 'diversity_loss', 'entropy_loss', 'stn_loss']
        title_suffix = "Original STN Prototype AE"
        
    elif model_type == "PrototypeAEBaseline":
        from models.prototype_ae_baseline import PrototypeAEBaseline, compute_baseline_losses
        model = PrototypeAEBaseline(NUM_PROTOTYPES, input_shape=(3, 20, 20)).cuda()
        compute_loss_fn = compute_baseline_losses
        loss_fields = ['recon_loss', 'diversity_loss', 'sparsity_loss']
        title_suffix = "Baseline Prototype AE"
        
    elif model_type == "ImprovedForcePrototypeAE":
        from models.improved_prototype_ae import ImprovedForcePrototypeAE, compute_improved_losses
        model = ImprovedForcePrototypeAE(NUM_PROTOTYPES, input_shape=(3, 20, 20)).cuda()
        compute_loss_fn = compute_improved_losses
        loss_fields = ['recon_loss', 'diversity_loss', 'entropy_loss', 'sparsity_loss', 'gini_coeff']
        title_suffix = "Improved Force Prototype AE"
        
    elif model_type == "ImprovedPrototypeSTNAE":
        from models.improved_prototype_ae_STN import ImprovedPrototypeSTNAE, compute_improved_stn_losses
        model = ImprovedPrototypeSTNAE(
            num_prototypes=NUM_PROTOTYPES, 
            input_shape=(3, 20, 20),
            share_stn=False
        ).cuda()
        compute_loss_fn = compute_improved_stn_losses
        loss_fields = ['recon_loss', 'diversity_loss', 'entropy_loss', 'sparsity_loss', 'gini_coeff', 'stn_loss', 'theta_diversity_loss']
        title_suffix = "Improved STN Prototype AE"
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    return {
        'model': model,
        'compute_loss_fn': compute_loss_fn,
        'loss_fields': loss_fields,
        'title_suffix': title_suffix,
        'model_type': model_type
    }

def train_prototypes(model_type="ImprovedPrototypeSTNAE", progressive_stn=False):
    dataset = TactileForcesDataset(DATA_ROOT, DATA_CATEGORIES)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)

    # è·å–æ¨¡å‹é…ç½®
    config = get_model_config(model_type)
    model = config['model']
    compute_loss_fn = config['compute_loss_fn']
    loss_fields = config['loss_fields']
    title_suffix = config['title_suffix']
    
    print(f"ä½¿ç”¨æ¨¡å‹: {model_type}")
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # æ¸è¿›å¼STNè®­ç»ƒç­–ç•¥
    if progressive_stn and model_type == "ImprovedPrototypeSTNAE" and not model.share_stn:
        print(f"ğŸ”„ å¯ç”¨æ¸è¿›å¼STNè®­ç»ƒç­–ç•¥:")
        print(f"  - å‰ {int(EPOCHS * 0.6)} ä¸ªepoch: åŒæ­¥æ›´æ–°æ‰€æœ‰STNå‚æ•°")
        print(f"  - å {int(EPOCHS * 0.4)} ä¸ªepoch: ç‹¬ç«‹æ›´æ–°å„STNå‚æ•°")
        return train_prototypes_progressive_stn(model, compute_loss_fn, loss_fields, title_suffix, dataset, loader)
    
    # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œå¸¦æƒé‡è¡°å‡
    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ç§»é™¤verboseå‚æ•°ä»¥å…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10
    )

    best_loss = float('inf')
    patience_counter = 0
    patience = 20  # æ—©åœpatience
    
    # è®°å½•æŸå¤±å†å²ç”¨äºç»˜åˆ¶æ›²çº¿ - åŠ¨æ€é€‚é…ä¸åŒæ¨¡å‹çš„æŸå¤±å­—æ®µ
    loss_history = {'epoch': [], 'total_loss': []}
    for field in loss_fields:
        loss_history[field] = []

    for epoch in range(1, EPOCHS + 1):
        model.train()
        total_loss = 0
        # åŠ¨æ€åˆå§‹åŒ–æŒ‡æ ‡ç´¯ç§¯å™¨
        metrics_sum = {field: 0 for field in loss_fields}
        
        for batch in tqdm(loader, desc=f"Epoch {epoch}/{EPOCHS}"):
            batch = batch.cuda()
            
            # å‰å‘ä¼ æ’­ - é€‚é…ä¸åŒæ¨¡å‹çš„è¾“å‡º
            if model_type in ["ImprovedPrototypeSTNAE", "PrototypeAutoencoder"]:
                # STNæ¨¡å‹è¿”å›4ä¸ªå€¼
                recon, weights, transformed_protos, thetas = model(batch)
                
                # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æŸå¤±å‡½æ•°å‚æ•°
                if model_type == "ImprovedPrototypeSTNAE":
                    loss, metrics = compute_loss_fn(
                        batch, recon, weights, transformed_protos, thetas,
                        diversity_lambda=0.5, entropy_lambda=0.1, 
                        sparsity_lambda=0.03, stn_reg_lambda=0.01
                    )
                else:  # PrototypeAutoencoder
                    loss, metrics = compute_loss_fn(
                        batch, recon, weights, transformed_protos, thetas,
                        diversity_lambda=0.1, entropy_lambda=10.0, stn_reg_lambda=0.05
                    )
            else:
                # éSTNæ¨¡å‹è¿”å›3ä¸ªå€¼
                recon, weights, protos = model(batch)
                
                if model_type == "ImprovedForcePrototypeAE":
                    loss, metrics = compute_loss_fn(
                        batch, recon, weights, protos,
                        diversity_lambda=1.0, entropy_lambda=0.1, sparsity_lambda=0.01
                    )
                else:  # PrototypeAEBaseline
                    loss, metrics = compute_loss_fn(
                        batch, recon, weights, protos,
                        diversity_lambda=0.1, sparsity_lambda=0.01
                    )
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # ç´¯ç§¯æŸå¤±å’ŒæŒ‡æ ‡
            batch_size = batch.size(0)
            total_loss += loss.item() * batch_size
            for k, v in metrics.items():
                metrics_sum[k] = metrics_sum.get(k, 0) + v * batch_size
        
        # è®¡ç®—å¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
        avg_loss = total_loss / len(dataset)
        avg_metrics = {k: v/len(dataset) for k, v in metrics_sum.items()}
        
        # å­¦ä¹ ç‡è°ƒåº¦
        prev_lr = optimizer.param_groups[0]['lr']
        scheduler.step(avg_loss)
        current_lr = optimizer.param_groups[0]['lr']
        
        # æ‰‹åŠ¨è®°å½•å­¦ä¹ ç‡å˜åŒ–
        if current_lr != prev_lr:
            print(f"å­¦ä¹ ç‡ä» {prev_lr:.2e} é™åˆ° {current_lr:.2e}")
        
        # æ‰“å°è®­ç»ƒä¿¡æ¯
        print(f"Epoch {epoch} Loss: {avg_loss:.4f} LR: {current_lr:.2e}")
        print("Metrics:", " ".join(f"{k}: {v:.4f}" for k, v in avg_metrics.items()))
        
        # è®°å½•æŸå¤±å†å²
        loss_history['epoch'].append(epoch)
        loss_history['total_loss'].append(avg_loss)
        for key, value in avg_metrics.items():
            if key in loss_history:
                loss_history[key].append(value)
        
        # æ—©åœæ£€æŸ¥
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
            # ä¿å­˜æœ€å¥½çš„æ¨¡å‹
            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, "best_model.pt"))
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch}")
            break

    # ä¿å­˜æ¨¡å‹å’ŒåŸå‹
    prototypes_np = model.prototypes.detach().cpu().numpy()
    np.save(os.path.join(OUTPUT_DIR, "prototypes.npy"), prototypes_np)
    save_physicalXYZ_images(prototypes_np, OUTPUT_DIR)  # ä½¿ç”¨utilsç»Ÿä¸€å‡½æ•°
    torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, "prototype_model.pt"))

    # ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒæŸå¤±æ›²çº¿
    plot_loss_curves(
        loss_history, 
        save_path=os.path.join(OUTPUT_DIR, "training_loss_curves.png"),
        title=f"{title_suffix} Training Loss"
    )
    
    # ä¿å­˜æŸå¤±å†å²æ•°æ®
    np.save(os.path.join(OUTPUT_DIR, "loss_history.npy"), loss_history)

    # ä¿å­˜æ ·æœ¬æƒé‡å’Œå¯è§†åŒ–
    # ä¿å­˜æ ·æœ¬æƒé‡å¹¶ç”Ÿæˆåˆ†æå›¾è¡¨
    save_sample_weights_and_analysis(model, dataset, output_dir=OUTPUT_DIR)


def train_prototypes_progressive_stn(model, compute_loss_fn, loss_fields, title_suffix, dataset, loader):
    """
    æ¸è¿›å¼STNè®­ç»ƒç­–ç•¥ï¼šå‰40%åŒæ­¥ï¼Œå60%ç‹¬ç«‹
    """
    print("ğŸš€ å¼€å§‹æ¸è¿›å¼STNè®­ç»ƒ...")
    
    # è®¡ç®—é˜¶æ®µåˆ’åˆ†
    sync_epochs = int(EPOCHS * 0.6)
    independent_epochs = EPOCHS - sync_epochs
    
    # è®°å½•æŸå¤±å†å²
    loss_history = {'epoch': [], 'total_loss': []}
    for field in loss_fields:
        loss_history[field] = []
    
    best_loss = float('inf')
    
    # ==================== é˜¶æ®µ1: åŒæ­¥STNè®­ç»ƒ (å‰40%) ====================
    print(f"\nğŸ“ é˜¶æ®µ1: åŒæ­¥STNè®­ç»ƒ (1-{sync_epochs} epochs)")
    print("æ‰€æœ‰STNæ¨¡å—å…±äº«æ¢¯åº¦æ›´æ–°...")
    
    # é˜¶æ®µ1ä¼˜åŒ–å™¨ - ç¨é«˜å­¦ä¹ ç‡
    optimizer_sync = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)
    scheduler_sync = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer_sync, mode='min', factor=0.7, patience=8
    )
    
    for epoch in range(1, sync_epochs + 1):
        model.train()
        total_loss = 0
        metrics_sum = {field: 0 for field in loss_fields}
        
        # åŒæ­¥æ›´æ–°ç­–ç•¥ï¼šè®©æ‰€æœ‰STNæ¨¡å—çš„æ¢¯åº¦ä¿æŒä¸€è‡´
        for batch in tqdm(loader, desc=f"åŒæ­¥è®­ç»ƒ Epoch {epoch}/{sync_epochs}"):
            batch = batch.cuda()
            
            # å‰å‘ä¼ æ’­
            recon, weights, transformed_protos, thetas = model(batch)
            
            # è®¡ç®—æŸå¤±
            loss, metrics = compute_loss_fn(
                batch, recon, weights, transformed_protos, thetas,
                diversity_lambda=0.5, entropy_lambda=0.1, 
                sparsity_lambda=0.03, stn_reg_lambda=0.01
            )
            
            # åå‘ä¼ æ’­
            optimizer_sync.zero_grad()
            loss.backward()
            
            # ğŸ”‘ å…³é”®ï¼šåŒæ­¥æ‰€æœ‰STNæ¨¡å—çš„æ¢¯åº¦
            if hasattr(model, 'stn_modules'):
                sync_stn_gradients(model.stn_modules)
            
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer_sync.step()
            
            total_loss += loss.item()
            for field in loss_fields:
                if field in metrics:
                    metrics_sum[field] += metrics[field]
        
        # è®°å½•æœ¬epochç»“æœ
        avg_loss = total_loss / len(loader)
        avg_metrics = {field: metrics_sum[field] / len(loader) for field in loss_fields}
        
        loss_history['epoch'].append(epoch)
        loss_history['total_loss'].append(avg_loss)
        for field in loss_fields:
            loss_history[field].append(avg_metrics[field])
        
        scheduler_sync.step(avg_loss)
        
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, "best_model_sync.pt"))
        
        if epoch % 10 == 0:
            print(f'åŒæ­¥é˜¶æ®µ Epoch {epoch}, Loss: {avg_loss:.4f}')
            print(f'  ä¸»è¦æŒ‡æ ‡: recon={avg_metrics.get("recon_loss", 0):.4f}, '
                  f'stn={avg_metrics.get("stn_loss", 0):.4f}')
    
    print(f"âœ… é˜¶æ®µ1å®Œæˆï¼æœ€ä½³æŸå¤±: {best_loss:.4f}")
    
    # ==================== é˜¶æ®µ2: ç‹¬ç«‹STNè®­ç»ƒ (å60%) ====================
    print(f"\nğŸ“ é˜¶æ®µ2: ç‹¬ç«‹STNè®­ç»ƒ ({sync_epochs+1}-{EPOCHS} epochs)")
    print("å„STNæ¨¡å—ç‹¬ç«‹æ›´æ–°ï¼Œå­¦ä¹ ç‰¹å®šå˜æ¢...")
    
    # é˜¶æ®µ2ä¼˜åŒ–å™¨ - é™ä½å­¦ä¹ ç‡ï¼Œæ›´ç²¾ç»†è°ƒèŠ‚
    optimizer_independent = torch.optim.AdamW(model.parameters(), lr=LR*0.5, weight_decay=1e-4)
    scheduler_independent = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer_independent, mode='min', factor=0.8, patience=10
    )
    
    for epoch in range(sync_epochs + 1, EPOCHS + 1):
        model.train()
        total_loss = 0
        metrics_sum = {field: 0 for field in loss_fields}
        
        for batch in tqdm(loader, desc=f"ç‹¬ç«‹è®­ç»ƒ Epoch {epoch}/{EPOCHS}"):
            batch = batch.cuda()
            
            # å‰å‘ä¼ æ’­
            recon, weights, transformed_protos, thetas = model(batch)
            
            # è®¡ç®—æŸå¤± - å¢åŠ thetaå¤šæ ·æ€§æƒé‡
            loss, metrics = compute_loss_fn(
                batch, recon, weights, transformed_protos, thetas,
                diversity_lambda=0.5, entropy_lambda=0.1, 
                sparsity_lambda=0.03, stn_reg_lambda=0.008  # ç¨å¾®é™ä½STNæ­£åˆ™åŒ–
            )
            
            # åå‘ä¼ æ’­ - æ­£å¸¸ç‹¬ç«‹æ›´æ–°
            optimizer_independent.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer_independent.step()
            
            total_loss += loss.item()
            for field in loss_fields:
                if field in metrics:
                    metrics_sum[field] += metrics[field]
        
        # è®°å½•æœ¬epochç»“æœ
        avg_loss = total_loss / len(loader)
        avg_metrics = {field: metrics_sum[field] / len(loader) for field in loss_fields}
        
        loss_history['epoch'].append(epoch)
        loss_history['total_loss'].append(avg_loss)
        for field in loss_fields:
            loss_history[field].append(avg_metrics[field])
        
        scheduler_independent.step(avg_loss)
        
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, "best_model.pt"))
        
        if epoch % 10 == 0:
            print(f'ç‹¬ç«‹é˜¶æ®µ Epoch {epoch}, Loss: {avg_loss:.4f}')
            print(f'  ä¸»è¦æŒ‡æ ‡: recon={avg_metrics.get("recon_loss", 0):.4f}, '
                  f'theta_div={avg_metrics.get("theta_diversity_loss", 0):.4f}')
    
    print(f"âœ… æ¸è¿›å¼è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæœ€ä½³æŸå¤±: {best_loss:.4f}")
    
    # ä¿å­˜ç»“æœ
    prototypes_np = model.prototypes.detach().cpu().numpy()
    np.save(os.path.join(OUTPUT_DIR, "prototypes.npy"), prototypes_np)
    save_physicalXYZ_images(prototypes_np, OUTPUT_DIR)
    torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, "prototype_model.pt"))
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_loss_curves(
        loss_history, 
        save_path=os.path.join(OUTPUT_DIR, "training_loss_curves.png"),
        title=f"{title_suffix} Progressive Training Loss"
    )
    
    # æ·»åŠ é˜¶æ®µåˆ†ç•Œçº¿æ ‡è®°
    import matplotlib.pyplot as plt
    plt.figure(figsize=(12, 8))
    epochs = loss_history['epoch']
    plt.plot(epochs, loss_history['total_loss'], 'b-', linewidth=2, label='Total Loss')
    plt.axvline(x=sync_epochs, color='red', linestyle='--', alpha=0.7, 
                label=f'Phase Switch (Epoch {sync_epochs})')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title(f'{title_suffix} Progressive Training - Phase Transition')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(OUTPUT_DIR, "progressive_training_phases.png"), dpi=300, bbox_inches='tight')
    plt.close()
    
    np.save(os.path.join(OUTPUT_DIR, "loss_history.npy"), loss_history)
    save_sample_weights_and_analysis(model, dataset, output_dir=OUTPUT_DIR)
    
    return model, loss_history


def sync_stn_gradients(stn_modules):
    """
    åŒæ­¥å¤šä¸ªSTNæ¨¡å—çš„æ¢¯åº¦ï¼Œè®©å®ƒä»¬å­¦ä¹ ç›¸ä¼¼çš„å˜æ¢
    """
    if len(stn_modules) <= 1:
        return
    
    # è®¡ç®—æ‰€æœ‰STNæ¨¡å—å‚æ•°çš„å¹³å‡æ¢¯åº¦
    avg_grads = {}
    
    # æ”¶é›†æ‰€æœ‰æ¢¯åº¦
    for name, param in stn_modules[0].named_parameters():
        if param.grad is not None:
            grad_sum = param.grad.clone()
            count = 1
            
            # ç´¯åŠ å…¶ä»–æ¨¡å—çš„å¯¹åº”æ¢¯åº¦
            for i in range(1, len(stn_modules)):
                other_param = dict(stn_modules[i].named_parameters())[name]
                if other_param.grad is not None:
                    grad_sum += other_param.grad
                    count += 1
            
            avg_grads[name] = grad_sum / count
    
    # å°†å¹³å‡æ¢¯åº¦åˆ†é…ç»™æ‰€æœ‰æ¨¡å—
    for stn_module in stn_modules:
        for name, param in stn_module.named_parameters():
            if name in avg_grads and param.grad is not None:
                param.grad.copy_(avg_grads[name])


if __name__ == '__main__':
    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æˆ–ç¯å¢ƒå˜é‡é€‰æ‹©æ¨¡å‹ç±»å‹å’Œè®­ç»ƒç­–ç•¥
    import sys
    
    # é»˜è®¤å‚æ•°
    model_type = "ImprovedPrototypeSTNAE"
    progressive_stn = False
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        model_type = sys.argv[1]
    if len(sys.argv) > 2 and sys.argv[2].lower() in ['true', '1', 'progressive']:
        progressive_stn = True
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    log_suffix = "_progressive" if progressive_stn else ""
    sys.stdout = Logger(f"{OUTPUT_DIR}/train_forces_{model_type.lower()}{log_suffix}.log")
    
    # ======================= Report =======================
    print("=" * 60)
    print(f"Forces Prototype Discovery Configuration:")
    print(f"MODEL TYPE       = {model_type}")
    if model_type == "ImprovedPrototypeSTNAE":
        print(f"SHARE_STN        = False (ç‹¬ç«‹STN)")
        if progressive_stn:
            print(f"TRAINING MODE    = Progressive (å‰40%åŒæ­¥ï¼Œå60%ç‹¬ç«‹)")
        else:
            print(f"TRAINING MODE    = Standard (å…¨ç¨‹ç‹¬ç«‹)")
    print(f"DATA_ROOT        = {DATA_ROOT}")
    print("DATA_CATEGORIES:")
    for category in DATA_CATEGORIES:
        print(f"  - {category}")
    print(f"OUTPUT_DIR       = {OUTPUT_DIR}")
    print(f"NUM_PROTOTYPES   = {NUM_PROTOTYPES}")
    print(f"BATCH_SIZE       = {BATCH_SIZE}")
    print(f"EPOCHS           = {EPOCHS}")
    print(f"LR               = {LR}")
    print(f"START_FRAME      = {START_FRAME}")
    if progressive_stn:
        print(f"SYNC_EPOCHS      = {int(EPOCHS * 0.4)} (40%)")
        print(f"INDEPENDENT_EPOCHS = {int(EPOCHS * 0.6)} (60%)")
    print("=" * 60)
    
    train_prototypes(model_type=model_type, progressive_stn=progressive_stn)
